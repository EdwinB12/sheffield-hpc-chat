{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.document_loaders.web_base import _build_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load xml file for local file \n",
    "with open('sitemap.xml') as f:\n",
    "    sitemap = f.read()\n",
    "\n",
    "soup = BeautifulSoup(sitemap, features=\"xml\")\n",
    "sitemap_tags = soup.find_all(\"loc\")\n",
    "urls = [url.text for url in sitemap_tags]\n",
    "\n",
    "# Filter out urls that are not relevant\n",
    "urls = [url for url in urls if '/hpc/' in url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/index.html'}\n",
      "\n",
      "\n",
      "\n",
      "Using the HPC Systems\n",
      "If you have not used a High Performance Computing (HPC) cluster,\n",
      "the Linux operating system\n",
      "or even a command line before\n",
      "this is the place to start.\n",
      "This guide will get you set up using the University’s clusters\n",
      "(Stanage and Bessemer) fairly quickly.\n",
      "\n",
      "\n",
      "What is High Performance Computing?\n",
      "Introduction\n",
      "Who can use or should use a HPC cluster?\n",
      "What is a HPC cluster?\n",
      "When should I use HPC?\n",
      "When should I not use HPC?\n",
      "How do I get started?\n",
      "\n",
      "\n",
      "Getting an Account\n",
      "For Staff\n",
      "For Students\n",
      "Conditions for HPC access / usage\n",
      "\n",
      "\n",
      "Connecting to a cluster using SSH\n",
      "Connecting with a Password or SSH keys\n",
      "Establishing a SSH connection\n",
      "Suggested SSH clients\n",
      "What if I cannot use the VPN or I need a persistent long term connection\n",
      "What Next?\n",
      "\n",
      "\n",
      "Tips for working with MFA\n",
      "Reuse SSH connections\n",
      "SSH clients\n",
      "Putty\n",
      "\n",
      "\n",
      "Filestores\n",
      "Choosing the correct filestore\n",
      "Home directories\n",
      "Fastdata areas\n",
      "Shared (project) directories\n",
      "Scratch  directories\n",
      "Community areas for software\n",
      "How to check your quota usage\n",
      "If you exceed your filesystem quota\n",
      "Recovering files from snapshots\n",
      "\n",
      "\n",
      "Transferring files\n",
      "Transfers with SCP/SFTP\n",
      "How to download files directly to the cluster\n",
      "\n",
      "\n",
      "Job Submission and Control\n",
      "Introduction\n",
      "Key Concepts\n",
      "Job Submission / Control on Stanage & Bessemer\n",
      "Cluster job resource limits\n",
      "Advanced / Automated job submission and management\n",
      "Reference information and further resources\n",
      "\n",
      "\n",
      "Advanced Job Submission and Control\n",
      "Introduction\n",
      "Advanced Job Submission Methods\n",
      "\n",
      "\n",
      "Advanced Job Profiling and Analysis\n",
      "Accessing a Running Single-Node Slurm Batch job\n",
      "Accessing a Running Multi-Node Slurm Batch job\n",
      "\n",
      "\n",
      "Activating software using Environment Modules\n",
      "Overview and rationale\n",
      "Basic guide\n",
      "Searching for Modules\n",
      "Behind the scenes\n",
      "Convenient ways to set up your environment for different projects\n",
      "Managing your own module files\n",
      "Compiling software dependent on modules\n",
      "Module Command Reference\n",
      "\n",
      "\n",
      "Choosing appropriate compute resources\n",
      "Introduction\n",
      "Cluster choice\n",
      "Time Allocation Limits\n",
      "CPU Allocation Limits\n",
      "Memory Allocation Limits\n",
      "Filestore Limits / file store performance characteristics\n",
      "Special limits and alternative queues\n",
      "\n",
      "\n",
      "Installing software to the clusters\n",
      "General background prequisites\n",
      "Installing software from binaries\n",
      "Installing software by compiling from source\n",
      "Making installed software available to execute\n",
      "Why should I install from source?\n",
      "What alternative methods exist?\n",
      "\n",
      "\n",
      "Graphical Sessions (With Flight) on Stanage\n",
      "Usage instructions\n",
      "\n",
      "\n",
      "HPC Gateway Service\n",
      "Service description\n",
      "Access conditions\n",
      "Overview of the connection process\n",
      "Specific usage examples\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/what-is-hpc.html'}\n",
      "\n",
      "\n",
      "\n",
      "What is High Performance Computing?\n",
      "\n",
      "Introduction\n",
      "If you are new to high-performance computing your first question is most likely:\n",
      "“What is high-performance computing (HPC)?”\n",
      "\n",
      "\n",
      "\n",
      "The old Iceberg system server racks.\n",
      "\n",
      "\n",
      "A good definition is:\n",
      "\n",
      "High Performance Computing most generally refers to the practice of aggregating\n",
      "computing power/resources in a way that delivers much higher performance / resource\n",
      "than one could get out of a typical desktop computer.\n",
      "\n",
      "In practice, the term “HPC” is difficult to define well in one concise and simple definition\n",
      "due to the varied characteristics of the many software and hardware combinations\n",
      "which constitutes HPC.\n",
      "It should also be noted that HPC is usually used to mean either\n",
      "“high performance computing” or “high performance computer” which is usually\n",
      "clear from the context of its use.\n",
      "Users of high performance computing will typically dispatch their workloads to\n",
      "a HPC cluster which is a large computer aggregrated from many smaller computers.\n",
      "It should be noted however that using a HPC cluster is not the only way run\n",
      "these kinds of workloads as other platforms can be used,\n",
      "e.g. dedicated high performance workstations.\n",
      "\n",
      "\n",
      "\n",
      "Who can use or should use a HPC cluster?\n",
      "Typically a researcher’s local desktop or laptop has between 8 to 16 GB of memory,\n",
      "4 to 16 CPU cores and a few TB of disk space where HPC clusters will usually have\n",
      "at least an order of magnitude more resources, if not more. Their research workloads\n",
      "may be too large for their own machine due to not having enough memory, disk space\n",
      "or it may simply take too long to run computations in a timely manner.\n",
      "The thought of using a HPC cluster might seem intimidating to researchers, but in\n",
      "principle any researcher with a workload that is too large for their desktop or\n",
      "laptop should access HPC resources and can do so via a small amount of HPC training.\n",
      "This is all provided free of charge at the University of Sheffield for University\n",
      "of Sheffield academics by IT Services.\n",
      "\n",
      "\n",
      "\n",
      "What is a HPC cluster?\n",
      "A HPC cluster is a large computer composed of a collection of many smaller\n",
      "separate servers (computers) which are called nodes. Nodes are typically connected\n",
      "to one another with a fast interconnect, such as\n",
      "Omnipath\n",
      "on the Stanage cluster, in order to pass data in between them very\n",
      "quickly.\n",
      "\n",
      "\n",
      "\n",
      "A general cluster schematic, source: Yale Center for Research Computing\n",
      "\n",
      "\n",
      "Both general HPC clusters and those at Sheffield are composed of:\n",
      "\n",
      "login nodes (also known as headnodes) where users login, edit, upload / download\n",
      "files but should not run any intensive programs and cannot load software modules.\n",
      "compute nodes where user jobs are ran.\n",
      "large memory nodes which are compute nodes with increased amounts of RAM available.\n",
      "GPU nodes which are compute nodes with multiple GPUs available.\n",
      "reserved nodes which are typically purchased by a department or research group\n",
      "for their exclusive use.\n",
      "storage nodes / the attached filestores which provide the\n",
      "cluster storage areas.\n",
      "\n",
      "All cluster nodes are equipped with the same types of components as a consumer laptop\n",
      "or desktop, i.e. CPU cores, memory and disk space but differ as these components\n",
      "are drastically improved in terms of quantity, quality, redundancy and magnitude\n",
      "of compute power.\n",
      "All user work is dispatched to a cluster using a tool called a job scheduler.\n",
      "A job scheduler is a tool used to manage, submit and fairly queue users’\n",
      "jobs in the shared environment of a HPC cluster. A cluster will normally use a\n",
      "single scheduler and allow a user to request either an immediate interactive job,\n",
      "or a queued batch job.\n",
      "\n",
      "Login nodes\n",
      "The login nodes are your gateway to the cluster from which you view/edit/upload\n",
      "files and dispatch jobs to the compute nodes. These nodes will be accessible over\n",
      "SSH however running any intensive programs is forbidden and cluster software is\n",
      "not available for this reason.\n",
      "\n",
      "\n",
      "Compute nodes\n",
      "The compute nodes are where your jobs will run. The compute nodes mount all\n",
      "shared filesystems making software and files available for your jobs irrespective\n",
      "of the node/s in which they run. These nodes are not accessible over SSH and\n",
      "direct access via methods other than the scheduler is forbidden.\n",
      "\n",
      "\n",
      "Large memory nodes\n",
      "Large memory nodes are identical to normal compute nodes but have additional\n",
      "memory capacity. Thus, they have the capability to run more memory-intensive\n",
      "shared memory parallel processing (OpenMP) jobs. Jobs requiring sufficiently large\n",
      "amounts of RAM will automatically be dispatched to these nodes.\n",
      "\n",
      "\n",
      "GPU nodes\n",
      "GPU nodes are principally the same as compute nodes but with the addition of\n",
      "special compute optimised GPUs typically used for accelerating modelling of\n",
      "engineering applications of AI / machine learning tasks. Jobs requiring the use\n",
      "of GPUs must specify this requirement as part of their resource request.\n",
      "\n",
      "\n",
      "Reserved or specialist nodes\n",
      "Reserved / specialist nodes are nodes not available in the public ‘free’ queues\n",
      "and have either standard or non-standard specifications. They are reserved for the\n",
      "exclusive use of the purchasing users/department/research groups. Some of these\n",
      "compute nodes will be purpose built to accelerate specific tasks and may have\n",
      "different job scheduler queue configurations.\n",
      "Jobs requiring or desiring the use of specialist nodes must specify this requirement\n",
      "as part of their job resource request.\n",
      "At The University of Sheffield, the available reserved nodes and their details can be\n",
      "found on their specific pages for the Bessemer cluster.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When should I use HPC?\n",
      "You should use HPC resources when your research workflows stand to benefit\n",
      "from HPC or where a research workflow is not possible to run on your own\n",
      "available resources.\n",
      "The following are typical use cases when it may be beneficial to request\n",
      "access to a HPC cluster:\n",
      "\n",
      "Computations need much more memory than what is available on your computer.\n",
      "The same program needs to be run many times\n",
      "(usually on different input data).\n",
      "The program that you use takes too long to run, but it can be run faster\n",
      "in parallel with multiple cores, typically utilising MPI\n",
      "or shared memory parallel processing (OpenMP).\n",
      "\n",
      "You need access to a GPU (your program needs to be written in a way\n",
      "that allows it to use your GPU or it uses GPU acceleration).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When should I not use HPC?\n",
      "\n",
      "Working with sensitive data\n",
      "\n",
      "Danger\n",
      "The High Performance Computing service must not be used to store or process any restricted or sensitive data.\n",
      "\n",
      "Examples of restricted or sensitive data include medical records, personal information, financial data, commercially sensitive data etc…\n",
      "If you are unsure whether you are working with restricted or sensitive data, do not transfer data to the HPC clusters without first\n",
      "discussing your requirements with IT Services.\n",
      "Due to the complexity of the multi-user High Performance Computing service,\n",
      "the service is not currently certified as being compliant with the\n",
      "Cyber Essentials, Cyber Essentials Plus or ISO 27001 schemes/standards.\n",
      "This is unlikely to change in future.\n",
      "Extra care should always be taken when dealing with sensitive information; if you are in any doubt about\n",
      "the sensitivity of information, or how it should be handled, then please contact IT Services\n",
      "research-it@sheffield.ac.uk for advice.\n",
      "\n",
      "Hint\n",
      "If processing of restricted or sensitive data is required, we recommend use of the Secure Data Service\n",
      "and getting in contact with the Secure Data Service team\n",
      "\n",
      "\n",
      "\n",
      "Low volume workloads\n",
      "Learning to use the HPC clusters will take both time, training and effort.\n",
      "If you have a low volume of work then investing time in learning to use Linux,\n",
      "shell scripting and other skills for using HPC clusters may be better spent\n",
      "elsewhere.\n",
      "\n",
      "\n",
      "Low volume, low memory serial workloads\n",
      "Using a HPC cluster is not a magic bullet that will make any workload run faster.\n",
      "Any workflows that only run on a single core (serial processing) and do not need\n",
      "large amounts of memory are likely to run slower on the HPC than on most modern\n",
      "desktops and laptop computers.\n",
      "If you run a low volume of serial jobs you will\n",
      "likely find your own computer would have completed these quicker.\n",
      "\n",
      "\n",
      "For training purposes\n",
      "University HPC clusters are used to facilitate large computational workloads and\n",
      "are not usually used as a training aid / facility. Exceptions may be made for HPC\n",
      "specific training with prior engagement with the HPC staff.\n",
      "HPC staff can help you optimise your workflows and software for use on the\n",
      "HPC clusters, but they cannot teach you how to use your program in great detail\n",
      "nor train you on the basic usage of a program.\n",
      "At The University of Sheffield, research training needs should be addressed via\n",
      "training courses provided by IT Services’ Research and Innovation team\n",
      "(VPN must be turned on),\n",
      "Research Software Engineering\n",
      "or Departmental / research group resources. PhD students can also make use of their\n",
      "doctoral development program\n",
      "to attend specific courses from any department that are relevant to their\n",
      "PhD training and development.\n",
      "\n",
      "\n",
      "For non-legitimate or non-research purposes\n",
      "University HPC clusters are provided to facilitate legitimate research workloads.\n",
      "Inappropriate usage of cluster resources, e.g. mining crypto-currency, hosting\n",
      "web services, abusing file storage for personal files, accessing files or software\n",
      "to which a user is not entitled or other non-legitimate usage will likely result in\n",
      "an investigation under the host organisation’s IT Code of Practice.\n",
      "Account sharing is also not permitted and any users/parties caught sharing accounts\n",
      "will also likely result in an investigation under the host organisation’s IT Code\n",
      "of Practice.\n",
      "The University of Sheffield IT Code of Practice can be found at the following link:\n",
      "https://www.sheffield.ac.uk/it-services/codeofpractice/core\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How do I get started?\n",
      "Potential users should first register and attend training courses RIT 101 and RIT 102 on\n",
      "IT Services’ Research and Innovation course details and registration information website\n",
      "(VPN must be turned on) and should then request an account.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/accounts.html'}\n",
      "\n",
      "\n",
      "\n",
      "Getting an Account\n",
      "Before you can start using the clusters you must first request HPC access. HPC access allows usage\n",
      "of all of our clusters (Stanage and Bessemer).\n",
      "\n",
      "\n",
      "For Staff\n",
      "HPC access is made available for staff by request via the HPC access\n",
      "request form which can be found at the\n",
      "IT Service Desk Self Service Portal under the option\n",
      "Service Request Forms.\n",
      "We recommend that staff also complete the\n",
      "HPC Driving License test. (The VPN must be connected for\n",
      "access.)\n",
      "\n",
      "\n",
      "For Students\n",
      "The following categories of students can also have HPC access with\n",
      "the permission of their supervisors:\n",
      "\n",
      "Research Postgraduates\n",
      "Taught Postgraduates - for project work\n",
      "Undergraduates 3rd & 4th year  - for project work\n",
      "\n",
      "To be granted HPC access, all students must first pass the\n",
      "HPC Driving License test. (The VPN must be connected for\n",
      "access.)\n",
      "Following this, they should request that their academic supervisor fill in the HPC access\n",
      "request form which can be found at the\n",
      "IT Service Desk Self Service Portal under the option\n",
      "Service Request Forms.\n",
      "Once you have been granted HPC access by the service desk, you can use your normal\n",
      "Sheffield login credentials to connect to the clusters.\n",
      "For further details on how to connect to the clusters please read the\n",
      "section on how to connect to the clusters.\n",
      "\n",
      "\n",
      "\n",
      "Conditions for HPC access / usage\n",
      "\n",
      "Important\n",
      "You must not share your account credentials or allow others to run jobs using your account.\n",
      "Any misuse of accounts will be investigated in accordance with\n",
      "the University of Sheffield’s\n",
      "IT Code of Practice.\n",
      "\n",
      "As a multiple user shared system, our clusters have some additional considerations in comparison\n",
      "to other IT Services provided at the University of Sheffield. Although we have strived to ensure\n",
      "that a robust baseline of security is in place suitable for the HPC community, we are aware\n",
      "that regulatory bodies and our research partners are asking for increasingly stringent security\n",
      "controls that the HPC may not have in place.\n",
      "\n",
      "Warning\n",
      "The High Performance Computing service should not be used to store or process sensitive information.\n",
      "For example: medical records, personal information, financial data, commercially sensitive or\n",
      "any other form of restricted data without first\n",
      "discussing your requirements with IT Services.\n",
      "\n",
      "Due to the complexity of the multi-user High Performance Computing service,\n",
      "the service is not currently certified as being compliant with the\n",
      "Cyber Essentials, Cyber Essentials Plus or ISO 27001 schemes/standards.\n",
      "This is unlikely to change in future.\n",
      "Extra care should always be taken when dealing with sensitive information; if you are in any doubt about\n",
      "the sensitivity of information, or how it should be handled, then please contact IT Services\n",
      "research-it@sheffield.ac.uk for advice.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/myapps.html'}\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/connecting.html'}\n",
      "\n",
      "\n",
      "\n",
      "Connecting to a cluster using SSH\n",
      "\n",
      "Hint\n",
      "Usernames to connect with all HPC services will be the same as those you use to login to MUSE not the prefix on your email address.\n",
      "\n",
      "The most versatile way to run commands and submit jobs on one of the clusters is to\n",
      "use a mechanism called SSH,\n",
      "which is a common way of remotely logging in to computers\n",
      "running the Linux operating system.\n",
      "To connect to another machine using SSH you need to\n",
      "have a SSH client program installed on your machine.\n",
      "macOS and Linux come with a command-line (text-only) SSH client pre-installed.\n",
      "On Windows there are various SSH clients you can use,\n",
      "including Windows Terminal.\n",
      "\n",
      "Warning\n",
      "The University Connect for China (UCC) is not the same service as the SSL VPN service and will not grant access to the HPC clusters.\n",
      "Users of the UCC must disconnect the UCC and connect to the SSL VPN in order to connect to the HPC clusters.\n",
      "\n",
      "\n",
      "Warning\n",
      "Eduroam no longer grants direct access to the clusters. If using Eduroam, you must keep the  VPN\n",
      "connected at all times while using the clusters.\n",
      "\n",
      "Valid methods of connecting to the University clusters using SSH (or the related protocols SCP and SFTP) include:\n",
      "\n",
      "Connecting while in a campus building using wired ethernet;\n",
      "Connecting while on campus using Eduroam or off campus after establishing a VPN connection (required);\n",
      "Connecting while off campus without a VPN connection using the HPC SSH gateway.\n",
      "\n",
      "Connecting using a password or SSH public key authentication will determine whether Multifactor Authentication (MFA) will be mandatory during the login process.\n",
      "The authentication requirements per cluster are summarised below:\n",
      "\n",
      "\n",
      "Cluster\n",
      "From campus or via VPN\n",
      "From off campus and without a VPN connection\n",
      "\n",
      "\n",
      "\n",
      "Bessemer\n",
      "Password + DUO MFA or public key\n",
      "Not permitted (unless using the HPC SSH gateway service)\n",
      "\n",
      "Stanage\n",
      "Password + DUO MFA or public key\n",
      "Not permitted (unless using the HPC SSH gateway service)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Connecting with a Password or SSH keys\n",
      "\n",
      "With a PasswordWith SSH keysIf connecting using your password, MFA will be mandatory. The standard University DUO MFA is utilised.\n",
      "\n",
      "StanageBessemerOn the Stanage cluster, when you connect you will be prompted via a push notification to your DUO device to approve access\n",
      "or must enter a one-time code from your University provided hardware token which is associated with your DUO account.\n",
      "ssh te1st@stanage.shef.ac.uk\n",
      "Password:\n",
      "Verification code:\n",
      "Last login: Wed Apr 12 17:09:24 2023 from r.x.y.z\n",
      "*****************************************************************************\n",
      "*                           Stanage HPC cluster                             *\n",
      "*                       The University Of Sheffield                         *\n",
      "*                       https://docs.hpc.shef.ac.uk                         *\n",
      "*                                                                           *\n",
      "*               Unauthorised use of this system is prohibited.              *\n",
      "*****************************************************************************\n",
      "[te1st@login1 [stanage] ~]$\n",
      "\n",
      "\n",
      "If you have not setup your University DUO MFA, please follow the steps published at: https://www.sheffield.ac.uk/it-services/mfa/set-mfa\n",
      "On the Bessemer cluster, when you connect you will be prompted via a push notification to your DUO device to approve access\n",
      "or must enter a one-time code from your University provided hardware token which is associated with your DUO account.\n",
      "ssh te1st@bessemer.shef.ac.uk\n",
      "Password:\n",
      "Verification code:\n",
      "Last login: Wed Apr 12 17:09:24 2023 from r.x.y.z\n",
      "*****************************************************************************\n",
      "*                           Bessemer HPC cluster                             *\n",
      "*                       The University Of Sheffield                         *\n",
      "*                       https://docs.hpc.shef.ac.uk                         *\n",
      "*                                                                           *\n",
      "*               Unauthorised use of this system is prohibited.              *\n",
      "*****************************************************************************\n",
      "[te1st@bessemer-login1 ~]$\n",
      "\n",
      "\n",
      "If you have not setup your University DUO MFA, please follow the steps published at: https://www.sheffield.ac.uk/it-services/mfa/set-mfa\n",
      "In addition, if you do not have MFA enabled on your account then you will not be able to login from off campus without using the VPN.\n",
      "\n",
      "If connecting using SSH public keys, the following policy applies around their use:\n",
      "\n",
      "Policy on the use of SSH public key authentication:\n",
      "\n",
      "All access to TUOS HPC systems via SSH public/private keypairs should use private keys that were encrypted with a passphrase at creation time.\n",
      "All SSH private keys used to access TUOS HPC systems must be never be decrypted and stored as plaintext on any computer, at any time.\n",
      "Public key access should be from single-user machines (not shared machines) without good reason.\n",
      "SSH agent forwarding should not be used without good reason.\n",
      "Unencrypted private keys should not be stored on TUOS HPC systems.\n",
      "\n",
      "\n",
      "To discuss exceptions to this policy please contact research-it@sheffield.ac.uk\n",
      "\n",
      "\n",
      "\n",
      "Establishing a SSH connection\n",
      "\n",
      "Hint\n",
      "Usernames to connect with all HPC services will be the same as those you use to login to MUSE not the prefix on your email address, and should always be entered in lowercase.\n",
      "\n",
      "Once you have a terminal open run the following command to\n",
      "log in to a cluster:\n",
      "\n",
      "Windows/LinuxmacOSssh -X YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk\n",
      "\n",
      "\n",
      "ssh -X YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "If this fails then:\n",
      "\n",
      "Check that your XQuartz is up to date then try again or\n",
      "Try again with -Y instead of -X\n",
      "\n",
      "\n",
      "\n",
      "Here you need to:\n",
      "\n",
      "replace YOUR_USERNAME with your IT Services username, which should be all lowercase (e.g., te1st).\n",
      "replace CLUSTER_NAME with stanage or bessemer.\n",
      "\n",
      "After typing in this command hit enter to start connecting at which point you will be prompted\n",
      "for your username, password and then with a Duo MFA prompt.\n",
      "This should give you a session resembling the one below:\n",
      "\n",
      "StanageBessemer[te1st@login1 [stanage] ~]$\n",
      "\n",
      "\n",
      "At this prompt if you would like an interactive session you can type:\n",
      "srun --pty bash -i\n",
      "\n",
      "\n",
      "Like this:\n",
      "[te1st@login1 [stanage] ~]$ srun --pty bash -i\n",
      "\n",
      "\n",
      "Which will start an interactive session, which supports graphical applications resembling the below:\n",
      "[te1st@node001 [stanage] ~]$\n",
      "\n",
      "\n",
      "[te1st@bessemer-login1 ~]$\n",
      "\n",
      "\n",
      "At this prompt if you would like an interactive session you can type:\n",
      "srun --pty bash -i\n",
      "\n",
      "\n",
      "Like this:\n",
      "[te1st@bessemer-login1 ~]$ srun --pty bash -i\n",
      "\n",
      "\n",
      "Which will start an interactive session, which supports graphical applications resembling the below:\n",
      "[te1st@bessemer-node001 ~]$\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "When you login to a cluster you reach one of two login nodes.\n",
      "You should not run applications on the login nodes.\n",
      "Running the interactive job command, srun --pty bash -i (Stanage & Bessemer), gives you an interactive terminal\n",
      "on one of the many worker nodes in the clusters.\n",
      "\n",
      "Running commands from a terminal (from the command-line) may initially be\n",
      "unfamiliar to Windows users but this is the recommended approach for\n",
      "running commands on Sheffield HPC clusters as\n",
      "it is the idiomatic way of interfacing with the Linux clusters.\n",
      "\n",
      "\n",
      "Suggested SSH clients\n",
      "\n",
      "SSH client software on Windows\n",
      "For Windows PCs, we recommend the use of Windows Terminal which is available on the University’s managed desktops by default. If not already installed on your device, Windows Terminal can be found on the Microsoft Store.\n",
      "\n",
      "Setting up Profiles in Windows Terminal.\n",
      "\n",
      "For personal systems you can download and install the Installer edition of MobaXterm.\n",
      "\n",
      "Setting up Profiles in MobaXterm.\n",
      "\n",
      "\n",
      "Caution\n",
      "We discourage the use of Warp Terminal when accessing our HPC clusters.\n",
      "This is due to multiple environment-related issues encountered by users who connect via Warp.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SSH client software on Mac OS/X and Linux\n",
      "Linux and macOS (OS X) both typically come with a command-line SSH client pre-installed.\n",
      "If you are using macOS and want to be able to run graphical applications on the clusters then\n",
      "you need to install the latest version of the XQuartz X Windows server.\n",
      "Open a terminal (e.g. Gnome Terminal on Linux or Terminal on macOS) and then go to Establishing a SSH connection.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What if I cannot use the VPN or I need a persistent long term connection\n",
      "Direct SSH access to the HPC clusters from off campus is not possible without the use of VPN. However\n",
      "if you are unable to use VPN we also provide an SSH gateway service to allow off-site SSH access to our HPC clusters.\n",
      "\n",
      "Note\n",
      "\n",
      "Access to the HPC SSH gateway service requires that you have an existing HPC account.\n",
      "You must additionally request access to the HPC SSH gateway by emailing research-it@sheffield.ac.uk including a justification for your request.\n",
      "If the cluster access can be handled via the usage of the SSL VPN without undue effort, your request will not be granted.\n",
      "\n",
      "\n",
      "For more information see HPC Gateway Service Details.\n",
      "\n",
      "\n",
      "What Next?\n",
      "Now you have connected to a cluster,\n",
      "you can look at how to submit jobs on the Job Submission and Control page or\n",
      "look at the software installed on\n",
      "Stanage and Bessemer.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/mfa.html'}\n",
      "\n",
      "\n",
      "\n",
      "Tips for working with MFA\n",
      "Here are some techniques you can use to reduce the number of times that you need to reauthenticate to our HPC systems.\n",
      "\n",
      "Reuse SSH connections\n",
      "Many SSH clients can reuse existing SSH sessions for new connections without the need to reconnect.  Some\n",
      "clients also allow sessions to persist temporarily after you have closed all your terminal windows to allow\n",
      "you to easily reconnect for a short time without having to reauthenticate.\n",
      "\n",
      "\n",
      "SSH clients\n",
      "SSH has a built in functionality to reuse existing connections for new sessions.  You can enable this feature by adding the following config\n",
      "to your ~/.ssh/config file on your local PC:\n",
      "\n",
      "StanageBessemerHost stanage.shef.ac.uk\n",
      "ServerAliveInterval 30\n",
      "ServerAliveCountMax 4\n",
      "ControlMaster auto\n",
      "ControlPath ~/.ssh/sockets/%r@%h-%p\n",
      "ControlPersist 600\n",
      "\n",
      "\n",
      "Host bessemer.shef.ac.uk\n",
      "ServerAliveInterval 30\n",
      "ServerAliveCountMax 4\n",
      "ControlMaster auto\n",
      "ControlPath ~/.ssh/sockets/%r@%h-%p\n",
      "ControlPersist 600\n",
      "\n",
      "\n",
      "\n",
      "You will need to create the directory ~/.ssh/sockets before running ssh.  The ControlPersist option allows you to specify how long (in seconds) your SSH connection\n",
      "should persist after you have closed all your existing sessions.  During this time you can start a new session without re-authenticating.\n",
      "\n",
      "Danger\n",
      "If you configure your SSH client to maintain connections ensure that your client PC is kept locked whenever\n",
      "you leave it unattended.\n",
      "\n",
      "\n",
      "Warning\n",
      "If you are temporarily disconnected from the network you may find that your SSH session does not immediately detect the failure.  You can delete the\n",
      "control socket created in ~/.ssh/sockets in order to clear the session and reconnect.  You should not use this option when running SSH commands on remote systems.\n",
      "\n",
      "\n",
      "\n",
      "Putty\n",
      "You can configure Putty to Share SSH connections if possible via the SSH option in the Connection Category when configuring a new connection.\n",
      "As long as your existing connection remains active you can start new sessions without re-authenticating by using Duplicate Session command to start new sessions.\n",
      "Other applications which use Putty for SSH connections can also re-use your existing connection without needing to reauthenticate.\n",
      "\n",
      "Warning\n",
      "If you perform a large file transfer over a shared session you may find that other sessions sharing the same connection become less responsive.\n",
      "\n",
      "\n",
      "TMUX/screen\n",
      "TMUX and screen are available on the HPC login nodes and\n",
      "can be used to run multiple shell sessions within a single SSH session.\n",
      "For examples of using TMUX to manage multiple sessions see the following RSE blog post: tmux: remote terminal management and multiplexing\n",
      "\n",
      "\n",
      "Transferring files\n",
      "If you need to transfer data from your local PC to a research shared directory you can directly access the data from your local PC without using MFA, instead of transferring\n",
      "the files via the HPC.\n",
      "For more info on how to do this see our Research Storage documentation .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/filestore.html'}\n",
      "\n",
      "\n",
      "\n",
      "Filestores\n",
      "Every HPC user has access to up to five different storage areas:\n",
      "\n",
      "Home directories: per-user storage\n",
      "Fastdata areas: high-performance shared filesystem for temporary data - optimised for reading/writing large files from multiple nodes and threads simultaneously\n",
      "Shared (project) directories: per-PI shared storage areas (snapshotted and backed-up) for project data - can be accessed from non-HPC machines too\n",
      "Scratch  directories: per-node temporary storage - useful for reading/writing lots of small files within one job\n",
      "Community areas for software: cluster-wide storage areas to allow users to share software.\n",
      "\n",
      "The storage areas differ in terms of:\n",
      "\n",
      "the amount of space available;\n",
      "whether they are available from multiple nodes;\n",
      "whether they are shared between clusters;\n",
      "whether the underlying storage system is performant if reading/writing large files;\n",
      "whether the underlying storage system is performant if reading/writing small files;\n",
      "frequency of storage snapshotting,\n",
      "whether storage is mirrored\n",
      "and the maximum duration data can be retained for;\n",
      "whether they handle permissions like a typical Linux filesystem.\n",
      "\n",
      "At present none provide encryption at rest.\n",
      "\n",
      "\n",
      "Choosing the correct filestore\n",
      "To make a quick assessment of what storage area is likely to best fulfil your needs, please take a look at the provided decision tree below:\n",
      "\n",
      "Warning\n",
      "This decision tree only provides a quick assessment, please check the full details of each filestore before committing to using them for your work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "StanageBessemerWhich cluster?Sheffield HPC Cluster Storage Selection decision tree:Does your job read or write lots of small files?Does your job read or write lots of small files?/scratchDoes your job read or write large or small files?Does your job read or write large or small files?/fastdataWarning: no backups, separate area Warning: file age limits and no backups.Are you running a job or storing input / output data?Are you running a job or storing input / output data?Running a jobRunning a jobYesYesLargeLargeNoNoDo you need to share your data with other users?Storing data/home/homeSmallSmallResearch shared storage area/homeNoYesDo you need to share your data with other users?Research shared storage areaStoring dataNoYesQuota: 50GB or 300k filesBackup and snapshotsImportant DetailsImportant DetailsQuota: none.Quota: none. per node and files deleted after job stops.Do you need a larger quota than 50GB?Do you need a larger quota than 100GB?No/homeQuota: 100GB No backups or snapshotsNo\n",
      "\n",
      "\n",
      "Home directories\n",
      "All users have a home directory on each system:\n",
      "\n",
      "StanageBessemerHome filestore area details\n",
      "\n",
      "\n",
      "Path\n",
      "Type\n",
      "Quota per user\n",
      "Shared between system login and worker nodes?\n",
      "Shared between systems?\n",
      "\n",
      "\n",
      "\n",
      "/users/$USER\n",
      "NFS\n",
      "50 GB or 300000 files\n",
      "Yes\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Where $USER is the user’s username.\n",
      "See also: How to check your quota usage and * If you exceed your filesystem quota.\n",
      "Home filestore backups and snapshots details\n",
      "\n",
      "Warning\n",
      "Snapshotting is not enabled for home areas and these areas are not backed up.\n",
      "\n",
      "Home filestore area details\n",
      "\n",
      "\n",
      "Path\n",
      "Type\n",
      "Quota per user\n",
      "Shared between system login and worker nodes?\n",
      "Shared between systems?\n",
      "\n",
      "\n",
      "\n",
      "/home/$USER\n",
      "NFS\n",
      "100GB\n",
      "Yes\n",
      "No\n",
      "\n",
      "\n",
      "\n",
      "Where $USER is the user’s username.\n",
      "See also: How to check your quota usage and * If you exceed your filesystem quota.\n",
      "Home filestore backups and snapshots details\n",
      "\n",
      "\n",
      "Frequency of snapshotting\n",
      "Snapshots retained\n",
      "\n",
      "\n",
      "\n",
      "Every 4 hours\n",
      "10 most recent\n",
      "\n",
      "Every night\n",
      "Last 7 days\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Frequency of mirrored backups\n",
      "Backups retained\n",
      "\n",
      "\n",
      "\n",
      "Every 4 hours\n",
      "6 most recent\n",
      "\n",
      "Every night\n",
      "28 most recent\n",
      "\n",
      "\n",
      "\n",
      "See also: Recovering files from snapshots.\n",
      "\n",
      "\n",
      "Note\n",
      "As you can see in the above tabs the full path to your home directory is different depending on the cluster you are on:\n",
      "\n",
      "\n",
      "Cluster\n",
      "Path\n",
      "\n",
      "\n",
      "\n",
      "Stanage\n",
      "/users/$USER\n",
      "\n",
      "Bessemer\n",
      "/home/$USER\n",
      "\n",
      "\n",
      "\n",
      "To ensure that your code is compatible with both clusters, we suggest using the symbols “~” or “$HOME” to represent the home directory. This approach ensures that the correct path is used regardless of the cluster you are working on, making your code more portable and agnostic to the specific cluster environment.\n",
      "\n",
      "StanageBessemer$ echo $HOME\n",
      "/users/te1st\n",
      "$ echo ~\n",
      "/users/te1st\n",
      "\n",
      "\n",
      "$ echo $HOME\n",
      "/home/te1st\n",
      "$ echo ~\n",
      "/home/te1st\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Fastdata areas\n",
      "Fastdata areas are optimised for large file operations.\n",
      "These areas are Lustre filesystems.\n",
      "They are faster than Home directories and Shared (project) directories when dealing with larger files but\n",
      "are not performant when reading/writing lots of small files\n",
      "(Scratch  directories are ideal for reading/writing lots of small temporary files within jobs).\n",
      "An example of how slow it can be for large numbers of small files is detailed here.\n",
      "There are separate fastdata areas on each cluster:\n",
      "\n",
      "StanageBessemerFastdata filestore area details\n",
      "\n",
      "\n",
      "Path\n",
      "Type\n",
      "Quota per user\n",
      "Filesystem capacity\n",
      "Shared between systems?\n",
      "Network bandwith per link\n",
      "\n",
      "\n",
      "\n",
      "/mnt/parscratch/\n",
      "Lustre\n",
      "No limits\n",
      "2 PiB\n",
      "No\n",
      "100Gb/s (Omni-Path)\n",
      "\n",
      "\n",
      "\n",
      "Managing your files in fastdata areas\n",
      "We recommend users create their own personal folder in the /fastdata area.  As this doesn’t exist by default, you can create it with safe permissions by running the command:\n",
      "mkdir /mnt/parscratch/users/$USER\n",
      "chmod 700 /mnt/parscratch/users/$USER\n",
      "\n",
      "\n",
      "By running the command above, your area will only be accessible to you. If desired, you could have a more sophisticated sharing scheme with private and fully public directories:\n",
      "mkdir /mnt/parscratch/users/$USER\n",
      "mkdir /mnt/parscratch/users/$USER/public\n",
      "mkdir /mnt/parscratch/users/$USER/private\n",
      "\n",
      "chmod 755 /mnt/parscratch/users/$USER\n",
      "chmod 755 /mnt/parscratch/users/$USER/public\n",
      "chmod 700 /mnt/parscratch/users/$USER/private\n",
      "\n",
      "\n",
      "Note however that the public folder in this instance will be readable to all users!\n",
      "Fastdata filestore backups and snapshots details\n",
      "\n",
      "Warning\n",
      "Snapshotting is not enabled for home areas and these areas are not backed up.\n",
      "\n",
      "File locking\n",
      "As of September 2020 POSIX file locking is enabled on all Lustre filesystems.\n",
      "Prior to this the lack of file locking support on the University’s Lustre filesystems caused problems for certain workflows/applications\n",
      "(e.g. for programs that create/use SQLite databases).\n",
      "User Quota management\n",
      "\n",
      "Warning\n",
      "There are no automated quota controls in the Stanage fastdata areas and the Stanage fastdata area currently has no automatic file deletion process.\n",
      "We reserve the right to prevent unfair use of this area by users and will manually assess user’s usage and establish a dialogue\n",
      "with users who are using unfair amounts of this area on a regular basis.\n",
      "We also reserve the right to take measures to ensure the continuing functionality of this area which could include scheduled removal of user’s files\n",
      "(after informing the user of the scheduled removal).\n",
      "\n",
      "Fastdata filestore area details\n",
      "\n",
      "\n",
      "Path\n",
      "Type\n",
      "Quota per user\n",
      "Filesystem capacity\n",
      "Shared between systems?\n",
      "Network bandwith per link\n",
      "\n",
      "\n",
      "\n",
      "/fastdata\n",
      "Lustre\n",
      "No limits\n",
      "460 TB\n",
      "No\n",
      "25Gb/s Ethernet\n",
      "\n",
      "\n",
      "\n",
      "Managing your files in fastdata areas\n",
      "We recommend users create their own personal folder in the /fastdata area.  As this doesn’t exist by default, you can create it with safe permissions by running the command:\n",
      "mkdir /fastdata/$USER\n",
      "chmod 700 /fastdata/$USER\n",
      "\n",
      "\n",
      "By running the command above, your area will only be accessible to you. If desired, you could have a more sophisticated sharing scheme with private and fully public directories:\n",
      "mkdir /fastdata/$USER\n",
      "mkdir /fastdata/$USER/public\n",
      "mkdir /fastdata/$USER/private\n",
      "\n",
      "chmod 755 /fastdata/$USER\n",
      "chmod 755 /fastdata/$USER/public\n",
      "chmod 700 /fastdata/$USER/private\n",
      "\n",
      "\n",
      "Note however that the public folder in this instance will be readable to all users!\n",
      "Fastdata filestore backups and snapshots details\n",
      "\n",
      "Warning\n",
      "Snapshotting is not enabled for fastdata areas and these areas are not backed up.\n",
      "\n",
      "Automatic file deletion\n",
      "\n",
      "Warning\n",
      "There are no quota controls in fastdata areas but\n",
      "older files are automatically deleted:\n",
      "a report of files older than 60 days is regularly generated,\n",
      "the owners of these files are then notified by email then\n",
      "a week after the email(s) are sent the identified files are deleted.\n",
      "We reserve the right to change this policy without warning in order to ensure efficient running of the service.\n",
      "It is important to therefore not use fastdata areas for long-term storage and\n",
      "copy important data from these areas to areas suitable for longer-term storage (Home directories or Shared (project) directories).\n",
      "\n",
      "You can use the lfs  command to find out which files in a fastdata directory are older than a certain number of days and hence approaching the time of deletion.\n",
      "For example, if your username is te1st then you can find files 50 or more days old using:\n",
      "lfs find -ctime +50 /fastdata/te1st\n",
      "\n",
      "\n",
      "File locking\n",
      "As of September 2020 POSIX file locking is enabled on all Lustre filesystems.\n",
      "Prior to this the lack of file locking support on the University’s Lustre filesystems caused problems for certain workflows/applications\n",
      "(e.g. for programs that create/use SQLite databases).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Shared (project) directories\n",
      "Each PI at the University is entitled to request a free 10 TB storage area for sharing data with their group and collaborators.\n",
      "The capacity per area can be extended and additional shared areas can be purchased (both at a cost).\n",
      "After one of these project storage areas has been requested/purchased it can be accessed in two ways:\n",
      "\n",
      "as a Windows-style (SMB) file share on machines other than Stanage and Bessemer using \\\\uosfstore.shef.ac.uk\\shared\\;\n",
      "as a subdirectory of /shared on Stanage and Bessemer (you need to explicitly request HPC access when you order storage from IT Services).\n",
      "\n",
      "\n",
      "Danger\n",
      "The High Performance Computing service must not be used to store or process any restricted or sensitive data. Shared areas with this type of data\n",
      "are not permitted to be made available on the clusters.\n",
      "Research shared storage areas may already contain sensitive data, or may contain sensitive data stored by colleagues you are not aware of.\n",
      "Before requesting an area to be made available on the HPC clusters, you must ensure that they do not and will not contain any sensitive\n",
      "data for the life time of the area.\n",
      "\n",
      "\n",
      "Snapshotting and mirrored backups\n",
      "\n",
      "\n",
      "Frequency of snapshotting\n",
      "Snapshots retained\n",
      "Backed up onto separate storage system\n",
      "\n",
      "\n",
      "\n",
      "Every 4 hours\n",
      "10 most recent\n",
      "Yes\n",
      "\n",
      "Every night\n",
      "Last 7 days\n",
      "Yes\n",
      "\n",
      "\n",
      "\n",
      "See also: Recovering files from snapshots.\n",
      "\n",
      "\n",
      "Automounting\n",
      "Subdirectories beneath /shared are mounted on demand on the HPC systems:\n",
      "they may not be visible if you simply list the contents of the /shared directory but\n",
      "will be accessible if you cd (change directory) into a subdirectory e.g. cd /shared/my_group_file_share1.\n",
      "\n",
      "\n",
      "Specifics for each Cluster\n",
      "As our HPC cluster are each hosted in different datacentres the policy, configuration and accessibility of the shared areas varies. The infomation for each cluster is shown below:\n",
      "\n",
      "StanageBessemerShared research area mount availability\n",
      "On the Stanage cluster, shared research areas can be made available on all login nodes only, upon request.  This is because:\n",
      "\n",
      "The HPC nodes are hosted within a datacentre in Manchester distant from the shared research area filestores hosted within the University’s Sheffield datacentres.\n",
      "Network traffic between Stanage and the Sheffield Research Filestore is not encrypted when travelling between Sheffield and Manchester over the dedicated leased line network link.\n",
      "The leased line network link has 10Gb/s of bidirectional transfer available.\n",
      "\n",
      "Shared research area performance\n",
      "\n",
      "If you access a /shared directory stored in Sheffield from Stanage then you may experience slower performance, especially for small files.\n",
      "The comparatively slower network link for Stanage than Bessemer could result in very poor performance if mounted on all worker nodes. This is why shared research areas are only made available on login nodes.\n",
      "Stanage does not have a local shared research area filestore, thus no local shared research areas can be made.\n",
      "\n",
      "If you need to access a /shared area on Stanage please contact research-it@sheffield.ac.uk to arrange this.\n",
      "Shared research area mount availability\n",
      "On the Bessemer cluster shared research areas can be made available on all HPC nodes upon request.  This is because:\n",
      "\n",
      "The HPC nodes are hosted within a datacentre in Leeds distant from the shared research area filestores hosted within the University’s Sheffield datacentres.\n",
      "Network traffic between Bessemer and the Sheffield Research Filestore is not encrypted when travelling between Sheffield and Leeds over the JANET network.\n",
      "\n",
      "Shared research area performance\n",
      "\n",
      "If you access a /shared directory stored in Sheffield from Bessemer then you may experience slower performance, especially for small files.\n",
      "\n",
      "If file store performance is a concern, /shared areas can be created on Bessemer’s local shared research area filestores system to improve performance for file access on the Bessemer HPC cluster. Please note that access\n",
      "to a Bessemer local shared research area filestore area from a Sheffield based machine will have a similar performance decrease.\n",
      "If you need to access a /shared area on Bessemer please contact research-it@sheffield.ac.uk to arrange this.\n",
      "\n",
      "\n",
      "\n",
      "Permissions behaviour\n",
      "You may encounter strange permissions issues when running programs on HPC against the /shared areas\n",
      "e.g. chmod +x /shared/mygroup1/myprogram.sh fails.\n",
      "Here we try to explain why.\n",
      "Behind the scenes, the file server that provides this shared storage manages permissions using\n",
      "Windows-style ACLs\n",
      "(which can be set by area owners via the Research Storage management web interface.\n",
      "However, the filesystem is mounted on a Linux cluster using NFSv4 so the file server therefore requires\n",
      "a means for mapping Windows-style permissions to Linux ones.\n",
      "An effect of this is that the Linux mode bits for files/directories under /shared on the HPC systems\n",
      "are not always to be believed:\n",
      "the output of ls -l somefile.sh may indicate that a file is readable/writable/executable when\n",
      "the ACLs are what really determine access permissions.\n",
      "Most applications have robust ways of checking for properties such as executability but\n",
      "some applications can cause problems when accessing files/directories on /shared by naively checking permissions just using Linux mode bits:\n",
      "\n",
      "which:\n",
      "a directory under /shared may be on your path and\n",
      "you may be able to run a contained executable without prefixing it with a absolute/relative directory\n",
      "but which may fail to find that executable.\n",
      "Perl: scripts that check for executability of files on /shared using -x may fail\n",
      "unless Perl is explicitly told to test for file permissions in a more thorough way\n",
      "(see the mention of use filetest 'access' here).\n",
      "git: may complain that permissions have changed if\n",
      "a repository is simply moved to /shared/someplace from elsewhere on Stanage/Bessemer.\n",
      "As a workaround you can tell git to not to track Linux permissions for a single repository using\n",
      "git config core.filemode false or\n",
      "for all repositories using git config --global core.filemode false.\n",
      "\n",
      "Changing how attempts to change permissions are handled: each /shared area can be configured so that\n",
      "\n",
      "Attempts to change file/directory mode bits fail (e.g. chmod +x /shared/mygroup1/myprogram.sh fails) (default configuration per area) or\n",
      "Attempts to change file/directory mode bits appear to succeed (e.g. chmod +x /shared/mygroup1/myprogram.sh does not fail but also does not actually change any permissions on the underlying file server) (alternative configuration per area)\n",
      "\n",
      "If you would like to switch to using the second way of handling permissions for a particular /shared/ area then\n",
      "the Owner of this area should make a request via the Helpdesk.\n",
      "\n",
      "\n",
      "Further information\n",
      "The documentation for the /shared storage service includes information on:\n",
      "\n",
      "how access/permissions are managed\n",
      "how to create folders with associated permissions\n",
      "within /shared storage areas\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scratch  directories\n",
      "For jobs that need to read/write lots of small files the most performant storage will be\n",
      "the temporary storage on each node.\n",
      "This is because with Home directories, Fastdata areas and Shared (project) directories,\n",
      "each time a file is accessed the filesystem needs to request ownership/permissions information from another server\n",
      "and for small files these overheads are proportionally high.\n",
      "For the local temporary store, such ownership/permissions metadata is available on the local machine,\n",
      "thus it is faster when dealing with small files.\n",
      "As the local temporary storage areas are node-local storage and files/folders are deleted when jobs end:\n",
      "\n",
      "any data used by the job must be copied to the local temporary store when the jobs starts.\n",
      "any output data stored in the local temporary store must also be copied off to another area before the job finishes.\n",
      "(e.g. to Home directories).\n",
      "\n",
      "Further conditions also apply:\n",
      "\n",
      "Anything in the local temporary store area may be deleted periodically when the worker-node is idle.\n",
      "The local temporary store area is not backed up.\n",
      "There are no quotas for local temporary store storage.\n",
      "The local temporary store area uses the ext4 filesystem.\n",
      "\n",
      "\n",
      "Danger\n",
      "The local temporary store areas are temporary and have no backups. If you forget to copy your output data out of the\n",
      "local temporary store area before your job finishes, your data cannot be recovered!\n",
      "\n",
      "\n",
      "Specifics for each Cluster\n",
      "\n",
      "StanageBessemerThe scheduler will automatically create a per-job directory for you under /tmp.\n",
      "The name of this directory is stored in the $TMPDIR environment variable e.g.\n",
      "[te1st@login1 [stanage] ~]$   srun -c 1 --mem=4G --pty bash -i\n",
      "[te1st@node001 [stanage] ~]$  cd $TMPDIR\n",
      "[te1st@node001 [stanage] ~]$  pwd\n",
      "/tmp/job.2660172\n",
      "\n",
      "\n",
      "The scheduler will then clean up (delete) $TMPDIR at the end of your job,\n",
      "ensuring that the space can be used by other users.\n",
      "The scheduler will automatically create a per-job directory for you under /scratch.\n",
      "The name of this directory is stored in the $TMPDIR environment variable e.g.\n",
      "[te1st@bessemer-login1 ~]$  srun -c 1 --mem=4G --pty bash -i\n",
      "[te1st@bessemer-node001 ~]$ cd $TMPDIR\n",
      "[te1st@bessemer-node001 2660172]$ pwd\n",
      "/scratch/2660172\n",
      "\n",
      "\n",
      "The scheduler will then clean up (delete) $TMPDIR at the end of your job,\n",
      "ensuring that the space can be used by other users.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Community areas for software\n",
      "Most data that researchers want to share with their collaborators at the University should reside in Shared (project) directories.\n",
      "However, as mentioned in Permissions behaviour, these areas may not be ideal for storing executable software/scripts\n",
      "due to the way permissions are handled beneath /shared.\n",
      "Also, users may want to install software on the clusters that they want to be accessible by all cluster users.\n",
      "To address these two needs users can request the creation of a new directory beneath of the three directories listed below\n",
      "and if their request is granted they will be given write access to this area:\n",
      "\n",
      "\n",
      "System\n",
      "Path\n",
      "Type\n",
      "Software install guidelines\n",
      "Public index of areas\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "Stanage\n",
      "N/A\n",
      "N/A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bessemer\n",
      "/usr/local/community\n",
      "NFS\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Note that:\n",
      "\n",
      "Software installation should follow our installation guidelines where provided.\n",
      "Software installations must be maintained by a responsible owner.\n",
      "Software which is not actively maintained may be removed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to check your quota usage\n",
      "To find out your storage quota usage for your home directory\n",
      "you can use the quota command:\n",
      "\n",
      "StanageBessemer[te1st@login1 [stanage] ~]$ quota -u -s\n",
      "    Filesystem   space   quota   limit   grace   files   quota   limit   grace\n",
      "storage:/export/users\n",
      "                 3289M  51200M  76800M            321k*   300k    350k   none\n",
      "\n",
      "\n",
      "An asterisk (*) after your space or files usage indicates that you’ve exceeded a ‘soft quota’. You’re then given a grace period of several days to reduce your usage below this limit.\n",
      "Failing to do so will prevent you from using additional space or creating new files. Additionally, there is a hard limit for space and files that can never be exceeded, even temporarily (i.e. it has no grace period).\n",
      "In the above example we can see that the user has exceeded their soft quota for files (‘*’) but not their hard limit for files.  However, the grace period field reads ‘none’,\n",
      "which means the grace period for exceeding the soft quota has already expired.  The user must remove/move some files from their home directory before they can create/add any more files.\n",
      "Also, the user is a long way from exceeding their space soft quota.\n",
      "\n",
      "Tip\n",
      "To assess what is using up your quota within a given directory, you can make use of the ncdu module on Stanage.\n",
      "The ncdu utility will give you an interactive display of what files/folders are taking up storage in a given directory tree.\n",
      "\n",
      "[te1st@bessemer-node004 binary]$ quota\n",
      "\n",
      "Size  Used Avail Use%  Mounted on\n",
      "100G  100G    0G 100%  /home/te1st\n",
      "\n",
      "\n",
      "In the above, you can see that the quota was set to 100 gigabytes and all of this is in use which is likely to cause jobs to fail.\n",
      "To determine usage in a particular Shared (project) directories you can use the df command like so:\n",
      "[te1st@bessemer-node004 binary]$ df -h /shared/myproject1\n",
      "Filesystem                        Size  Used Avail Use% Mounted on\n",
      "172.X.X.X:/myproject1/myproject1   10T  9.1T  985G  91% /shared/myproject1\n",
      "\n",
      "\n",
      "\n",
      "Tip\n",
      "To assess what is using up your quota within a given directory, you can make use of the\n",
      "ncdu module on Bessemer. The ncdu utility will give you an\n",
      "interactive display of what files/folders are taking up storage in a given directory tree.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "If you exceed your filesystem quota\n",
      "If you reach your quota for your home directory then\n",
      "many common programs/commands may cease to work as expected or at all and\n",
      "you may not be able to log in.\n",
      "In addition, jobs may fail if you exceed your quota with a job making use of a Shared (project) directory.\n",
      "In order to avoid this situation it is strongly recommended that you:\n",
      "\n",
      "Check your quota usage regularly.\n",
      "Copy files that do not need to be backed up to a Fastdata area\n",
      "or remove them from the clusters completely.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Recovering files from snapshots\n",
      "\n",
      "StanageBessemerRecovery of files and folders on Stanage is not possible as the Stanage cluster does not currently have snapshots or backups.\n",
      "If you need help, please contact research-it@sheffield.ac.uk.\n",
      "Home directories and Shared (project) directories are regularly snapshotted.\n",
      "See above for details of the snapshot schedules per area.\n",
      "A subset of snapshots can be accessed by HPC users from the HPC systems themselves\n",
      "by explicitly browsing to hidden directories e.g.\n",
      "\n",
      "\n",
      "Storage area\n",
      "Parent directory of snapshots\n",
      "\n",
      "\n",
      "\n",
      "Home directory\n",
      "$HOME/.snapshot\n",
      "\n",
      "A Shared (project) directory\n",
      "/shared/myproject1/.snapshot\n",
      "\n",
      "\n",
      "\n",
      "From within per-snapshot directories you can access (read-only) copies of files/directories.\n",
      "This allows you to attempt recover any files you might have accidentally modified or deleted recently.\n",
      "Note that .snapshot directories are not visible when listing all hidden items within their parent directories\n",
      "(e.g. using ls -a $HOME):\n",
      "you need to explicitly cd into .snapshot directories to see/access them.\n",
      "If you need help, please contact research-it@sheffield.ac.uk.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/transferring-files.html'}\n",
      "\n",
      "\n",
      "\n",
      "Transferring files\n",
      "\n",
      "Warning\n",
      "As with all connections to the clusters, if you are not using a wired ethernet connection in a\n",
      "University campus building then you will need to turn on the VPN.\n",
      "\n",
      "To transfer files to/from the clusters you can:\n",
      "\n",
      "Use a program that supports one or both of the SCP and SFTP protocols to copy/move files to/from your own machine\n",
      "or from a remote machine to the cluster.\n",
      "Use a Research Storage fileshare as common storage directly\n",
      "accessible from your own machine and from the clusters.\n",
      "Use a program like curl or wget to download files directly to the clusters.\n",
      "Use a flight session on Stanage or interactive session on Bessemer, to open a Firefox browser and interactively download directly to clusters.\n",
      "\n",
      "\n",
      "Hint\n",
      "Downloading directly to the cluster may be 10x to 100x faster than doing a transfer\n",
      "from your local desktop or laptop (particularly if connecting remotely via VPN) as this will avoid using your local device’s\n",
      "internet connection which is likely a bottleneck.\n",
      "If you are able, you should make direct downloads to the cluster.\n",
      "\n",
      "\n",
      "\n",
      "Transfers with SCP/SFTP\n",
      "Secure copy protocol (SCP) is a protocol for securely transferring computer files between a local host and a\n",
      "remote host or between two remote hosts. It is based on the Secure Shell (SSH) protocol and the acronym typically\n",
      "refers to both the protocol and the command itself.\n",
      "Secure File Transfer Protocol (SFTP) is also a file transfer protocol. It is based on the\n",
      "FTP protocol with included SSH security components.\n",
      "\n",
      "Hint\n",
      "If you need to move large files (e.g. larger than a gigabyte) from one remote machine to the cluster you\n",
      "should SSH in to the computer hosting the files and use scp or rsync to transfer over to the other directly as this will\n",
      "usually be quicker and more reliable.\n",
      "If you cannot SSH into the remote machine, consider an alternative\n",
      "direct transfer method listed below.\n",
      "\n",
      "\n",
      "Using SCP in the terminal\n",
      "If your local machine has a terminal and the scp  (“secure copy”) command is available\n",
      "you can use it to make transfers of files or folders.\n",
      "Where below substitute CLUSTER_NAME with stanage or bessemer\n",
      "and YOUR_USERNAME with your cluster username, and should always be entered in lowercase.\n",
      "You should be prompted for your Duo MFA credentials after entering your password. Request a push notification or enter your passcode.\n",
      "To upload, you transfer from your local machine to the remote cluster:\n",
      "scp /path/to/file.txt YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/directory/\n",
      "\n",
      "\n",
      "To download, you transfer from the remote cluster to your local machine:\n",
      "scp YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/file.txt /path/to/directory/\n",
      "\n",
      "\n",
      "To copy a whole directory, we add the -r flag, for “recursive”\n",
      "scp -r YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/my_results /path/to/directory/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using Filezilla\n",
      "FileZilla is a cross-platform client available for Windows, MacOS and Linux for downloading\n",
      "and uploading files to and from a remote computer.\n",
      "Download and install the FileZilla client from https://filezilla-project.org. After installing and opening the program,\n",
      "there is a window with a file browser of your local system on the left hand side of the screen\n",
      "and when you connected to a cluster, your cluster files will appear on the right hand side.\n",
      "To connect to the cluster, we’ll just need make a new site and enter our credentials in the General tab:\n",
      "\n",
      "Caution\n",
      "By default Filezilla will save profiles in plaintext on your machine. You must ensure you use a master password to\n",
      "encrypt these credentials by changing the settings\n",
      "as shown in these instructions.\n",
      "\n",
      "You can create a new site by selecting file from top menu bar then site manager which will open a dialog similar to:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "After hitting the new site button you can enter your credentials in the general tab:\n",
      "\n",
      "Host: sftp://CLUSTER_NAME.shef.ac.uk (replace CLUSTER_NAME with stanage or bessemer)\n",
      "User: Your cluster username, which should always be entered in lowercase.\n",
      "Password: Your cluster password (leave blank and fill this interactively if on a shared machine.)\n",
      "Port: (leave blank to use the default port)\n",
      "Protocol: sftp\n",
      "Logon Type: Interactive\n",
      "\n",
      "In the transfer settings tab limit the number of simultaneous connections to 1.\n",
      "Save these details as a profile and then connect. You should be prompted for your Duo MFA credentials.\n",
      "Request a push notification or enter your passcode.  You will now see your remote files appear on the\n",
      "right hand side of the screen. This process can be repeated to save a profile for each cluster.\n",
      "You can drag-and-drop files between the left (local) and right (remote) sides of the screen to transfer files.\n",
      "\n",
      "\n",
      "Using rsync\n",
      "As you become more familiar with transferring files, you may find that the scp is limited. The rsync utility provides\n",
      "advanced features for file transfer and is typically faster compared to both scp and sftp. It is a utility for\n",
      "efficiently transferring and synchronizing files between storage locations including networked computers by comparing the\n",
      "modification times and sizes of files. The utility is particularly useful as it can also resume failed or partial file\n",
      "transfers by using the --append-verify flag.\n",
      "Many users find rsync is especially useful for transferring large and/or many files as well as creating synced\n",
      "backup folders.\n",
      "\n",
      "Caution\n",
      "It is easy to make mistakes with rsync and accidentally transfer files to the wrong location, sync in the wrong\n",
      "direction or otherwise accidentally overwrite files. To help you avoid this, you can first use the --dry-run flag for\n",
      "rsync to show you the changes it will make for a given command.\n",
      "\n",
      "\n",
      "Note\n",
      "Be cautious when specifying paths with or without trailing slashes.\n",
      "Ensure that you understand how rsync interprets these slashes to prevent unintended outcomes.\n",
      "\n",
      "\n",
      "rsync Behaviour with Trailing Slashes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "With Trailing Slash on Source Directory:\n",
      "rsync -av /source/directory/ /destination/directory\n",
      "\n",
      "\n",
      "\n",
      "When you use a trailing slash on the source directory it tells rsync to copy the contents of the source directory into the destination directory.\n",
      "\n",
      "Without Trailing Slash on Source Directory:\n",
      "rsync -av /source/directory /destination/directory\n",
      "\n",
      "\n",
      "\n",
      "When you don’t use a trailing slash on the source directory it tells rsync to copy the source directory itself and its contents into the destination directory.\n",
      "\n",
      "Trailing Slash on Destination Directory:\n",
      "rsync -av /source/directory/ /destination/directory/\n",
      "\n",
      "\n",
      "\n",
      "When you use a trailing slash on the destination directory it tells rsync to copy the source directory itself and its contents into the destination directory.\n",
      "\n",
      "Without Trailing Slash on Destination Directory:\n",
      "rsync -av /source/directory/ /destination/directory\n",
      "\n",
      "\n",
      "\n",
      "When you don’t use a trailing slash on the destination directory it tells rsync to copy the contents of the source directory into the destination directory.\n",
      "\n",
      "\n",
      "\n",
      "The rsync syntax is very similar to scp. To transfer to another computer with commonly used options,\n",
      "where below substitute CLUSTER_NAME with stanage or bessemer and YOUR_USERNAME with your cluster username, which should always be entered in lowercase.\n",
      "You should be prompted for your Duo MFA credentials after entering your password. Request a push notification or\n",
      "enter your passcode:\n",
      "rsync -avzP /path/to/file.iso YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/directory/\n",
      "\n",
      "\n",
      "The a (archive) option preserves file timestamps and permissions among other things;\n",
      "the v (verbose) option gives verbose output to help monitor the transfer;\n",
      "the z (compression) option compresses the file during transit to reduce size and transfer time;\n",
      "and the P (partial/progress) option preserves partially transferred files in case of an interruption\n",
      "and also displays the progress of the transfer.\n",
      "To recursively copy a directory, we can use the same options:\n",
      "rsync -avzP /path/to/isos/ YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/directory/\n",
      "\n",
      "\n",
      "This will copy the local directory and its contents under the specified directory on the remote system.\n",
      "If the trailing slash is omitted on the destination path, a new directory corresponding to the transferred\n",
      "directory (isos in the example) will not be created, and the contents of the source directory will be copied\n",
      "directly into the destination directory.\n",
      "As before with scp, to download from the cluster rather than upload simply reverse the source and destination:\n",
      "rsync -avzP YOUR_USERNAME@CLUSTER_NAME.shef.ac.uk:/path/to/isos /path/to/directory/\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How to download files directly to the cluster\n",
      "Downloading files directly to the cluster is usually the quickest and most efficient\n",
      "way of getting files onto the clusters. Using your home connection will be a significant\n",
      "speed bottleneck compared to large amounts of download bandwidth available on the clusters.\n",
      "Directly downloading to the cluster avoids this bottleneck!\n",
      "\n",
      "Using Firefox Browser\n",
      "Firefox browser can be used on both Stanage and Bessemer. This will allow you to interactively navigate the web,\n",
      "login to websites and download files as you would do locally.\n",
      "\n",
      "StanageBessemerGraphical desktop access to an interactive session can be achieved using\n",
      "Flight Desktop and TigerVNC .\n",
      "Once you have loaded the GUI desktop, open a terminal at the bottom of the screen\n",
      "and enter the command firefox, which will launch a browser.\n",
      "On Bessemer a Firefox GUI can be loaded directly.  This can be achieved\n",
      "by starting an interactive session with the srun --pty bash -i command and then\n",
      "opening firefox by running the same named command. For this to function correctly you\n",
      "must ensure that X11/GUI forwarding is enabled when connecting with SSH.\n",
      "\n",
      "\n",
      "\n",
      "Using wget / curl\n",
      "One of the most efficient ways to download files to the clusters is to use either the\n",
      "curl or wget commands to download directly.\n",
      "The syntax for these commands is as below:\n",
      "\n",
      "Downloading with wget\n",
      "wget https://software.github.io/program/files/myprogram.tar.gz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Downloading with curl\n",
      "curl -O https://software.github.io/program/files/myprogram.tar.gz\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Using Git\n",
      "The Git software and same named command can be used to download or synchronise a remote Git\n",
      "repository onto the clusters. This can be achieved by\n",
      "setting up Git\n",
      "and/or simply cloning the repository you desire.\n",
      "For example, cloning the source of the make software:\n",
      "[user@login1 make-git]$ git clone https://git.savannah.gnu.org/git/make.git\n",
      "Cloning into 'make'...\n",
      "remote: Counting objects: 16331, done.\n",
      "remote: Compressing objects: 100% (3434/3434), done.\n",
      "remote: Total 16331 (delta 12822), reused 16331 (delta 12822)\n",
      "Receiving objects: 100% (16331/16331), 5.07 MiB | 2.79 MiB/s, done.\n",
      "Resolving deltas: 100% (12822/12822), done.\n",
      "\n",
      "\n",
      "Git is installed on the clusters and can be used on any node and all\n",
      "commands\n",
      "such as push, pull etc… are supported.\n",
      "\n",
      "\n",
      "Using lftp\n",
      "\n",
      "Hint\n",
      "It is recommended that you use an alternative method than lftp if possible. Using\n",
      "lftp in the command line interface should be a last resort as it is a little\n",
      "difficult / confusing to use.\n",
      "\n",
      "lftp is a command-line program client for FTP, FTPS, FXP, HTTP, HTTPS, FISH, SFTP,\n",
      "BitTorrent, and FTP over HTTP proxy.\n",
      "If you need to login to an FTP server to\n",
      "make a direct download to a cluster, you can use the lftp client.\n",
      "\n",
      "Connecting with lftp\n",
      "\n",
      "Caution\n",
      "Where possible please connect with the ftps protocol as this protects your username\n",
      "and password from hackers performing man in the middle or sniffing attacks!\n",
      "\n",
      "Connecting to an FTP server can be achieved as follows:\n",
      "lftp ftps://ftp.remotehost.com\n",
      "\n",
      "\n",
      "When this connection is successful an lftp prompt will appear as follows:\n",
      "lftp ftp.remotehost.com:~>\n",
      "\n",
      "\n",
      "At this stage you can now login after being prompted for your password\n",
      "as follows:\n",
      "lftp ftp.remotehost.com:~> login username\n",
      "Password:\n",
      "\n",
      "\n",
      "At this stage directory listing and changing directory can be achieved using the\n",
      "ls and cd commands. By default these commands run on the remote server. To run\n",
      "these commands on the local machine simply prefix each command with an ! i.e.\n",
      "!ls and !cd.\n",
      "The get (download) and put (upload) commands can also be used.\n",
      "\n",
      "\n",
      "Downloading with lftp\n",
      "To download a file use the get command as follows:\n",
      "lftp username@ftp.remotehost.com/> get myfile.txt -o mydownloadedfile.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Uploading with lftp\n",
      "To upload a file use the put command as follows:\n",
      "lftp username@ftp.remotehost.com/> put myfile.txt -o myuploadedfile.txt\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/index.html'}\n",
      "\n",
      "\n",
      "\n",
      "Job Submission and Control\n",
      "\n",
      "\n",
      "\n",
      "Contents\n",
      "\n",
      "Job Submission and Control\n",
      "\n",
      "Introduction\n",
      "Key Concepts\n",
      "Job Submission / Control on Stanage & Bessemer\n",
      "Cluster job resource limits\n",
      "Advanced / Automated job submission and management\n",
      "Reference information and further resources\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Introduction\n",
      "As mentioned in the what is HPC section, HPC clusters like\n",
      "Bessemer and Stanage use a program called a scheduler to control and submit work to\n",
      "appropriate nodes.\n",
      "\n",
      "All user work is dispatched to a cluster using a tool called a job scheduler.\n",
      "A job scheduler is a tool used to manage, submit and fairly queue users’\n",
      "jobs in the shared environment of a HPC cluster. A cluster will normally use a\n",
      "single scheduler and allow a user to request either an immediate interactive job,\n",
      "or a queued batch job.\n",
      "\n",
      "Here at the University of Sheffield, on both Bessemer and Stanage we use the\n",
      "SLURM scheduler, which follows three basic principles:\n",
      "\n",
      "they allocate exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work,\n",
      "they provide a framework for starting, executing, and monitoring work on the set of allocated nodes,\n",
      "they arbitrate contention for resources by managing a queue of pending work.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Key Concepts\n",
      "\n",
      "Tip\n",
      "If you are not familiar with basic computer architecture we highly recommend reading our\n",
      "General Computer Architecture Quick Start page\n",
      "before continuing.\n",
      "\n",
      "When engaging with our documentation several concepts must be well understood with reference to\n",
      "schedulers and jobs which will be explained below:\n",
      "\n",
      "Types of Job\n",
      "There are two types of job on any scheduler, interactive and batch:\n",
      "Interactive jobs are ones where they are requested and immediately run providing the user\n",
      "with a bash shell (or a shell of their choosing) in which they can then run their software or\n",
      "scripts in.\n",
      "Typically only very few nodes in a HPC cluster are dedicated solely to interactive jobs and\n",
      "interactive jobs require the resources to be available instantenously as the request is made\n",
      "or the request will fail. This means that interactive requests cannot always be fulfilled,\n",
      "particularly when requesting multiple cores.\n",
      "Batch jobs are the other kind of job where a user prepares a batch submission script which\n",
      "both requests the resources for the job from the scheduler and contains the execution commands\n",
      "for a given program to run. On job submission, the scheduler will add it to the chosen queue and\n",
      "run your job when resources become available.\n",
      "Any task that can be executed without any user intervention while it is running can be submitted as\n",
      "a batch job. This excludes jobs that require a Graphical User Interface (GUI), however, many common\n",
      "GUI applications such as ANSYS or MATLAB can also be used without their GUIs.\n",
      "If you wish to use a cluster for interactive work and/or running applications like MATLAB or ANSYS\n",
      "using GUIs, you will need to request an interactive job from the scheduler.\n",
      "If you wish to use a cluster to dispatch a very large ANSYS model you will need to request\n",
      "batch job from the scheduler and prepare an appropriate batch script.\n",
      "\n",
      "Note\n",
      "Long running jobs should use the batch submission system rather than\n",
      "requesting an interactive session for a very long time. Doing this will\n",
      "lead to better cluster performance for all users.\n",
      "\n",
      "\n",
      "\n",
      "Queues and partitions\n",
      "Queues or partitions (in SLURM) are queues of jobs submitted to a scheduler for it to run.\n",
      "They can have an assortment of constraints such as job size limit, job time limit, users\n",
      "permitted to use it and some nodes will be configured to accept jobs only from certain queues\n",
      "e.g. Department specific nodes.\n",
      "\n",
      "\n",
      "All jobs are dispatchable\n",
      "When a user requests that a job, (either a batch or an interactive session), is\n",
      "ran on the cluster, the scheduler will run jobs from the queue based\n",
      "on a set of rules, priorities and availabilities.\n",
      "How and where a job can run are set when the job is requested based on the resource\n",
      "amounts requested as well as the chosen queue (assuming a user has permissions to use a queue.)\n",
      "This means that not all interactive jobs are possible as the resources may not be available. It\n",
      "also means that the amount of time it takes for any batch job to run is dependent on how large the job\n",
      "resource request is, which queue it is in, what resources are available in that queue and\n",
      "how much previous resource usage the user has. The larger a resource request is, the longer it will\n",
      "take to wait for those resources to become available and the longer it will take for subsequent jobs\n",
      "to queue as a result of the fair scheduling algorithm.\n",
      "\n",
      "\n",
      "Fair scheduling\n",
      "Job schedulers are typically configured to use a fair-share / wait time system. In short, the scheduler assesses\n",
      "your previous CPU time and memory time (consumption) to give a requested job a priority. Subsequently it uses how\n",
      "long your job has had to wait in order to bump up that priority. Once your job is the highest priority, the job will\n",
      "then run when the requested resources become available on the system. Your running total for CPU time / memory time usage\n",
      "will decay over time but in general the more resources you request and for longer, the lower your initial job priority\n",
      "gets and the longer you have to wait behind other people’s jobs.\n",
      "If you are seeing one job start and another immediately begin this is not an intentional chaining setting on the scheduler’s\n",
      "part. This is quite likely simply a reflection of your subsequent jobs waiting for resources to become available and it just so\n",
      "happens that your running job finishes freeing up the resources for the next.\n",
      "As a natural consequence of backfilling into any trapped resources - you may see small time, memory and core request jobs with a\n",
      "lower priority running before your own with a higher priority. This is because they are small enough to utilize the trapped resource\n",
      "before the job trapping those resources is finished. This is not unfair and it would be inefficient and irresponsible for us to\n",
      "intentionally block a job from running simply because the priority is lower than a larger job that won’t fit in that trapped resource.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Job Submission / Control on Stanage & Bessemer\n",
      "\n",
      "Tip\n",
      "The Stanage & Bessemer clusters have been configured with resource request limits.\n",
      "Please see our Choosing appropriate compute resources page for further information.\n",
      "\n",
      "\n",
      "Interactive Jobs\n",
      "SLURM uses a single command to launch interactive jobs:\n",
      "\n",
      "srun Standard SLURM command supporting graphical applications.\n",
      "\n",
      "Usage of the command is as follows:\n",
      "$ srun --pty bash -i\n",
      "\n",
      "\n",
      "You can configure the resources available to the interactive session by adding command line options.\n",
      "For example to start an interactive session with access to 16 GB of RAM:\n",
      "$ srun --mem=16G --pty bash -i\n",
      "\n",
      "\n",
      "To start a session with access to 2 cores, use either:\n",
      "$ srun --cpus-per-task=2 --pty bash -i #2 cores per task, 1 task and 1 node per job default. Preferred!\n",
      "$ srun --ntasks-per-node=2 --pty bash -i #2 tasks per node, 1 core per task and 1 node per job default.\n",
      "\n",
      "\n",
      "Please take care with your chosen options as usage in concert with other options\n",
      "can be multiplicative.\n",
      "A further explanation of why you may use the tasks options or cpus options can be found here.\n",
      "A table of common interactive job options is given below; any of these can be\n",
      "combined together to request more resources.\n",
      "\n",
      "\n",
      "Slurm Command\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "-t min or -t days-hh:mm:ss\n",
      "Specify the total maximum wall clock execution time for the job.\n",
      "The upper limit is 08:00:00. Note: these limits may differ\n",
      "for reservations /projects.\n",
      "\n",
      "--mem=xxG\n",
      "\n",
      "--mem=xxG is used to specify the maximum amount (xx)\n",
      "of real memory to be requested per node.\n",
      " If the real memory usage of your job exceeds this value\n",
      "multiplied by the number of cores / nodes you requested then your\n",
      "job will be killed.\n",
      "\n",
      "\n",
      "-c nn or --cpus-per-task=nn\n",
      " -c is cores per task, take care with your chosen\n",
      "number of tasks.\n",
      "\n",
      "--ntasks-per-node=nn\n",
      " --ntasks-per-node= is tasks per node, take care with your\n",
      "chosen number of cores per node. The default is one task per node,\n",
      "but note that other options can adjust the default of 1 core per task\n",
      "e.g. --cpus-per-task.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Rejoining an interactive job\n",
      "If we lose connection to an interactive job, we can use the sattach command which attaches to a running Slurm job step.\n",
      "Just keep in mind that sattach doesn’t work for external or batch steps, as they aren’t\n",
      "set up for direct attachment.\n",
      "Example:\n",
      "\n",
      "StanageBessemer[te1st@login1 [stanage] ~]$ squeue --me\n",
      "        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "        833300 interacti     bash   te1st  R      31:22      1 node001\n",
      "[te1st@login1 [stanage] ~]$ sattach 833300.0\n",
      "[te1st@node001 [stanage] ~]$ echo $SLURM_JOB_ID\n",
      "833300\n",
      "\n",
      "\n",
      "[te1st@bessemer-login1 ~]$ squeue --me\n",
      "        JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "        833300 interacti     bash   te1st  R      31:22      1 node001\n",
      "[te1st@bessemer-login1 ~]$ sattach 833300.0\n",
      "[te1st@bessemer-node001 ~]$ echo $SLURM_JOB_ID\n",
      "833300\n",
      "\n",
      "\n",
      "\n",
      "Here we attached to SLURM job 833300 step 0. For more information type man sattach\n",
      "\n",
      "\n",
      "Batch Jobs\n",
      "\n",
      "Tip\n",
      "Batch jobs have larger resource limits than interactive jobs! For guidance on what these\n",
      "limits are and how best to select resources please see our Choosing appropriate compute resources page.\n",
      "\n",
      "SLURM uses a single command to submit batch jobs:\n",
      "\n",
      "sbatch Standard SLURM command with no support for interactivity or graphical applications.\n",
      "\n",
      "The Slurm docs have a complete list of available sbatch options.\n",
      "The batch submission scripts are executed for submission as below:\n",
      "sbatch submission.sh\n",
      "\n",
      "\n",
      "Note the job submission number. For example:\n",
      "Submitted batch job 1226\n",
      "\n",
      "\n",
      "You can check your output log or error log file as below:\n",
      "cat JOB_NAME-1226.out\n",
      "\n",
      "\n",
      "There are numerous further options you can request in your batch submission files which are\n",
      "detailed below:\n",
      "Name your job submission:\n",
      "#SBATCH --job-name=JOB_NAME\n",
      "\n",
      "\n",
      "Specify a number of nodes:\n",
      "#SBATCH --nodes=1\n",
      "\n",
      "\n",
      "\n",
      "Warning\n",
      "Note that the Bessemer free queues do not permit the use of more than 1 node per job.\n",
      "\n",
      "Specify a number of tasks per node:\n",
      "#SBATCH --ntasks-per-node=4\n",
      "\n",
      "\n",
      "Specify a number of tasks:\n",
      "#SBATCH --ntasks=4\n",
      "\n",
      "\n",
      "Specify a number of cores per task:\n",
      "#SBATCH --cpus-per-task=4\n",
      "\n",
      "\n",
      "\n",
      "StanageBessemerRequest a specific amount of memory per node:\n",
      "#SBATCH --mem=16G\n",
      "\n",
      "\n",
      "Request a specific amount of memory per CPU core:\n",
      "#SBATCH --mem-per-cpu=16G\n",
      "\n",
      "\n",
      "Request a specific amount of memory per job:\n",
      "#SBATCH --mem=16G\n",
      "\n",
      "\n",
      "\n",
      "Specify the job output log file name:\n",
      "#SBATCH --output=output.%j.test.out\n",
      "\n",
      "\n",
      "Request a specific amount of time:\n",
      "#SBATCH --time=00:30:00\n",
      "\n",
      "\n",
      "Request job update email notifications:\n",
      "#SBATCH --mail-user=username@sheffield.ac.uk\n",
      "\n",
      "\n",
      "For the full list of the available options please visit the SLURM manual webpage for\n",
      "sbatch here: https://slurm.schedmd.com/sbatch.html\n",
      "Here is an example SLURM batch submission script that runs a fictitious program called foo:\n",
      "#!/bin/bash\n",
      "# Request 5 gigabytes of real memory (mem)\n",
      "#SBATCH --mem=5G\n",
      "\n",
      "# load the module for the program we want to run\n",
      "module load apps/gcc/foo\n",
      "\n",
      "# Run the program foo with input foo.dat\n",
      "# and output foo.res\n",
      "foo foo.dat foo.res\n",
      "\n",
      "\n",
      "Some things to note:\n",
      "\n",
      "The first line always needs to be #!/bin/bash (to tell the scheduler that this is a bash batch script).\n",
      "Comments start with a #.\n",
      "It is always best to fully specify job’s resources with your submission script.\n",
      "All Slurm Scheduler options start with #SBATCH\n",
      "You should use the SLURM option --ntasks=nn Number of “tasks”, for programs using distributed\n",
      "parallelism (MPI).\n",
      "You should use the SLURM option --ntasks-per-node=nn Number of “tasks per node”, for programs\n",
      "using distributed parallelism (MPI). Note that the Bessemer free queues do not\n",
      "permit the use of more than 1 node per job.\n",
      "You should use the SLURM option --cpus-per-task=nn Number of “cores per task”, for programs using\n",
      "shared memory parallelism.\n",
      "You will often require one or more module commands in your submission file to make programs and\n",
      "libraries available to your scripts. Many applications and libraries are available as modules on\n",
      "Bessemer and Stanage.\n",
      "\n",
      "Here is a more complex example that requests more resources:\n",
      "#!/bin/bash\n",
      "# Request 16 gigabytes of real memory (RAM) 4 cores *4G = 16\n",
      "#SBATCH --mem=16G\n",
      "# Request 4 cores\n",
      "#SBATCH --cpus-per-task=4\n",
      "# Email notifications to me@somedomain.com\n",
      "#SBATCH --mail-user=me@somedomain.com\n",
      "# Email notifications if the job fails\n",
      "#SBATCH --mail-type=FAIL\n",
      "# Change the name of the output log file.\n",
      "#SBATCH --output=output.%j.test.out\n",
      "# Rename the job's name\n",
      "#SBATCH --job-name=my_job\n",
      "\n",
      "\n",
      "# Load the modules required by our program\n",
      "module load compilers/gcc/5.2\n",
      "module load apps/gcc/foo\n",
      "\n",
      "# Set the OPENMP_NUM_THREADS environment variable to 4\n",
      "# This is needed to ensure efficient core usage.\n",
      "export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n",
      "\n",
      "# Run the program foo with input foo.dat\n",
      "# and output foo.res\n",
      "foo foo.dat foo.res\n",
      "\n",
      "\n",
      "\n",
      "Tip\n",
      "Bessemer currently supports running preemptable jobs. These are jobs which have been set to run in a reserved queue’s node when those nodes are idle. These reserved queues\n",
      "are typically private (research group-owned or dept-owned) nodes,\n",
      "but these resources will be reclaimed (and the associated jobs preempted) if\n",
      "members of those groups/departments submit jobs that can only start if those resources are repurposed.\n",
      "For more details on running preemptable jobs on Bessemer please see: Preemptable jobs\n",
      "\n",
      "\n",
      "\n",
      "Monitoring running Jobs\n",
      "There are two commands to monitor running and queued jobs:\n",
      "\n",
      "sstat\n",
      "squeue\n",
      "\n",
      "The squeue command is used to pull up information about jobs in the queue, by default this\n",
      "command will list the job ID, partition, username, job status, number of nodes, and name of nodes\n",
      "for all jobs queued or running within SLURM.\n",
      "Display all jobs queued on the system:\n",
      "$ squeue\n",
      "\n",
      "\n",
      "To limit this command to only display a single user’s jobs the --user flag can be used:\n",
      "$ squeue --user=$USER\n",
      "\n",
      "\n",
      "To limit this command to only display your own jobs, the --me flag can be used:\n",
      "$ squeue --me\n",
      "\n",
      "\n",
      "Further information without abbreviation can be shown by using the --long flag:\n",
      "$ squeue --me --long\n",
      "\n",
      "\n",
      "The squeue command also provides a method to calculate the estimated start time for a job by\n",
      "using the --start flag:\n",
      "$ squeue --me --start\n",
      "\n",
      "\n",
      "The accuracy of squeue --start estimates varies due to factors like queue dynamics,\n",
      "resource availability (affected by maintenance, node failures, etc), making it a guideline rather than a guarantee.\n",
      "When checking the status of a job you may wish to check for updates at a time interval. This can\n",
      "be achieved by using the --iterate flag and a number of seconds:\n",
      "$ squeue --me --start --iterate=n_seconds\n",
      "\n",
      "\n",
      "You can stop this command by pressing Ctrl + C.\n",
      "Example output:\n",
      "$ squeue\n",
      "        JOBID   PARTITION   NAME      USER  ST       TIME  NODES NODELIST(REASON)\n",
      "        1234567 interacti   bash   foo1bar   R   17:19:40      1 bessemer-node001\n",
      "        1234568 sheffield job.sh   foo1bar   R   17:21:40      1 bessemer-node046\n",
      "        1234569 sheffield job.sh   foo1bar  PD   17:22:40      1 (Resources)\n",
      "        1234570 sheffield job.sh   foo1bar  PD   16:47:06      1 (Priority)\n",
      "        1234571       gpu job.sh   foo1bar   R 1-19:46:53      1 bessemer-node026\n",
      "        1234572       gpu job.sh   foo1bar   R 1-19:46:54      1 bessemer-node026\n",
      "        1234573       gpu job.sh   foo1bar   R 1-19:46:55      1 bessemer-node026\n",
      "        1234574       gpu job.sh   foo1bar   R 1-19:46:56      1 bessemer-node026\n",
      "        1234575       gpu job.sh   foo1bar  PD       9:04      1 (ReqNodeNotAvail, UnavailableNodes:bessemer-node026)\n",
      "        1234576 sheffield job.sh   foo1bar  PD    2:57:24      1 (QOSMaxJobsPerUserLimit)\n",
      "\n",
      "\n",
      "States shown above indicate job states including running “R” and Pending “PD” with various\n",
      "reasons for pending states including a node (ReqNodeNotAvail) full of jobs and a user hitting\n",
      "the max limit for numbers of jobs they can run simultaneously in a QOS (QOSMaxJobsPerUserLimit).\n",
      "A list of the most relevant job states and reasons can be seen below:\n",
      "SLURM Job States:\n",
      "Jobs typically pass through several states in the course of their execution. The typical\n",
      "states are PENDING, RUNNING, SUSPENDED, COMPLETING, and COMPLETED.\n",
      "\n",
      "\n",
      "Status\n",
      "Code\n",
      "Explanation\n",
      "\n",
      "\n",
      "\n",
      "COMPLETED\n",
      "CD\n",
      "The job has completed successfully.\n",
      "\n",
      "COMPLETING\n",
      "CG\n",
      "The job is finishing but some processes are still active.\n",
      "\n",
      "CANCELLED\n",
      "CA\n",
      "Job was explicitly cancelled by the user or system administrator.\n",
      "\n",
      "FAILED\n",
      "F\n",
      "The job terminated with a non-zero exit code and failed to execute.\n",
      "\n",
      "PENDING\n",
      "PD\n",
      "The job is waiting for resource allocation. It will eventually run.\n",
      "\n",
      "PREEMPTED\n",
      "PR\n",
      "The job was terminated because of preemption by another job.\n",
      "\n",
      "RUNNING\n",
      "R\n",
      "The job currently is allocated to a node and is running.\n",
      "\n",
      "SUSPENDED\n",
      "S\n",
      "A running job has been stopped with its cores released to other jobs.\n",
      "\n",
      "STOPPED\n",
      "ST\n",
      "A running job has been stopped with its cores retained.\n",
      "\n",
      "OUT_OF_MEMORY\n",
      "OOM\n",
      "Job experienced out of memory error.\n",
      "\n",
      "TIMEOUT\n",
      "TO\n",
      "Job exited because it reached its walltime limit.\n",
      "\n",
      "NODE_FAIL\n",
      "NF\n",
      "Job terminated due to failure of one or more allocated nodes.\n",
      "\n",
      "\n",
      "\n",
      "A full list of job states can be found at:\n",
      "https://slurm.schedmd.com/squeue.html#SECTION_JOB-STATE-CODES\n",
      "SLURM Job Reasons:\n",
      "These codes identify the reason that a job is waiting for execution. A job may\n",
      "be waiting for more than one reason, in which case only one of those reasons is displayed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Reason Code\n",
      "Explanation\n",
      "\n",
      "\n",
      "\n",
      "Priority\n",
      "One or more higher priority jobs is in queue for running. Your job will eventually run.\n",
      "\n",
      "Dependency\n",
      "This job is waiting for a dependent job to complete and will run afterwards.\n",
      "\n",
      "Resources\n",
      "The job is waiting for resources to become available and will eventually run.\n",
      "\n",
      "InvalidAccount\n",
      "The job’s account is invalid. Cancel the job and rerun with correct account.\n",
      "\n",
      "InvaldQoS\n",
      "The job’s QoS is invalid. Cancel the job and rerun with correct account.\n",
      "\n",
      "QOSGrpMaxJobsLimit\n",
      "Maximum number of jobs for your job’s QoS have been met; job will run eventually.\n",
      "\n",
      "PartitionMaxJobsLimit\n",
      "Maximum number of jobs for your job’s partition have been met; job will run eventually.\n",
      "\n",
      "AssociationMaxJobsLimit\n",
      "Maximum number of jobs for your job’s association have been met; job will run eventually.\n",
      "\n",
      "JobLaunchFailure\n",
      "The job could not be launched. This may be due to a file system problem, invalid program name, etc.\n",
      "\n",
      "NonZeroExitCode\n",
      "The job terminated with a non-zero exit code.\n",
      "\n",
      "SystemFailure\n",
      "Failure of the Slurm system, a file system, the network, etc.\n",
      "\n",
      "TimeLimit\n",
      "The job exhausted its time limit.\n",
      "\n",
      "WaitingForScheduling\n",
      "No reason has been set for this job yet. Waiting for the scheduler to determine the appropriate reason.\n",
      "\n",
      "BadConstraints\n",
      "The job’s constraints can not be satisfied.\n",
      "\n",
      "\n",
      "\n",
      "A full list of job reasons can be found at:\n",
      "https://slurm.schedmd.com/squeue.html#SECTION_JOB-REASON-CODES\n",
      "The sstat command can be used to display status information about a user’s currently running\n",
      "jobs such as the CPU usage, task or node information and memory consumption.\n",
      "The command can be invoked as follows with a specific job ID:\n",
      "$ sstat --jobs=job-id\n",
      "\n",
      "\n",
      "And to display specific information you can use the --format flag to choose your output:\n",
      "$ sstat --jobs=job-id --format=var_1,var_2, ... , var_N\n",
      "\n",
      "\n",
      "A chart of some these variables are listed in the table below:\n",
      "\n",
      "sstat format variable names\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Variable\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "AveCPU\n",
      "Average (system + user) CPU time of all tasks in job.\n",
      "\n",
      "AveRSS\n",
      "Average resident set size of all tasks in job.\n",
      "\n",
      "AveVMSize\n",
      "Average Virtual Memory size of all tasks in job.\n",
      "\n",
      "JobID\n",
      "The id of the Job.\n",
      "\n",
      "MaxRSS\n",
      "Maximum resident set size of all tasks in job.\n",
      "\n",
      "MaxVMSize\n",
      "Maximum Virtual Memory size of all tasks in job.\n",
      "\n",
      "NTasks\n",
      "Total number of tasks in a job or step.\n",
      "\n",
      "\n",
      "\n",
      "A full list of variables for the --format flag can be\n",
      "found with the --helpformat flag or by visiting the slurm page on\n",
      "sstat.\n",
      "\n",
      "\n",
      "Stopping or cancelling Jobs\n",
      "Jobs can be stopped or cancelled using the scancel command:\n",
      "\n",
      "scancel\n",
      "\n",
      "You can stop jobs with the scancel command and the job’s ID\n",
      "(replacing job-id with the number):\n",
      "$ scancel job-id\n",
      "\n",
      "\n",
      "To cancel multiple jobs you can supply a comma separated list:\n",
      "$ scancel job-id1, job-id2, job-id3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Investigating finished Jobs\n",
      "Jobs which have already finished can be investigated using the seff script:\n",
      "\n",
      "seff\n",
      "\n",
      "The seff script can be used as follows with the job’s ID to give summary of important job info :\n",
      "$ seff job-id\n",
      "\n",
      "\n",
      "For example, on the Stanage cluster:\n",
      "$ seff 64626\n",
      "Job ID: 64626\n",
      "Cluster: stanage.alces.network\n",
      "User/Group: a_user/clusterusers\n",
      "State: COMPLETED (exit code 0)\n",
      "Nodes: 2\n",
      "Cores per node: 1\n",
      "CPU Utilized: 00:02:37\n",
      "CPU Efficiency: 35.68% of 00:07:20 core-walltime\n",
      "Job Wall-clock time: 00:03:40\n",
      "Memory Utilized: 137.64 MB (estimated maximum)\n",
      "Memory Efficiency: 1.71% of 7.84 GB (3.92 GB/core)\n",
      "\n",
      "\n",
      "Or in even more depth using the sacct command:\n",
      "\n",
      "sacct\n",
      "\n",
      "The sacct command can be used to display status information about a user’s historical\n",
      "jobs.\n",
      "The command can be used as follows with the job’s ID:\n",
      "$ sacct --jobs=job-id\n",
      "\n",
      "\n",
      "Or to view information about all of a specific user’s jobs:\n",
      "$ sacct --user=$USER\n",
      "\n",
      "\n",
      "By default the sacct command will only bring up information about the user’s job from the\n",
      "current day. By using the --starttime flag the command will look further back to the given\n",
      "date e.g. :\n",
      "$ sacct --user=$USER --starttime=YYYY-MM-DD\n",
      "\n",
      "\n",
      "Like the sstat command, the --format flag can be used to choose the command output:\n",
      "$ sacct --user=$USER --format=var_1,var_2, ... ,var_N\n",
      "\n",
      "\n",
      "\n",
      "sacct format variable names\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Variable\n",
      "Description\n",
      "\n",
      "\n",
      "\n",
      "Account\n",
      "The account the job ran under.\n",
      "\n",
      "AveCPU\n",
      "Average (system + user) CPU time of all tasks in job.\n",
      "\n",
      "AveRSS\n",
      "Average resident set size of all tasks in job.\n",
      "\n",
      "AveVMSize\n",
      "Average Virtual Memory size of all tasks in job.\n",
      "\n",
      "CPUTime\n",
      "Formatted (Elapsed time * CPU) count used by a job or step.\n",
      "\n",
      "Elapsed\n",
      "Jobs elapsed time formated as DD-HH:MM:SS.\n",
      "\n",
      "ExitCode\n",
      "The exit code returned by the job script or salloc.\n",
      "\n",
      "JobID\n",
      "The id of the Job.\n",
      "\n",
      "JobName\n",
      "The name of the Job.\n",
      "\n",
      "MaxRSS\n",
      "Maximum resident set size of all tasks in job.\n",
      "\n",
      "MaxVMSize\n",
      "Maximum Virtual Memory size of all tasks in job.\n",
      "\n",
      "MaxDiskRead\n",
      "Maximum number of bytes read by all tasks in the job.\n",
      "\n",
      "MaxDiskWrite\n",
      "Maximum number of bytes written by all tasks in the job.\n",
      "\n",
      "ReqCPUS\n",
      "Requested number of CPUs.\n",
      "\n",
      "ReqMem\n",
      "Requested amount of memory.\n",
      "\n",
      "ReqNodes\n",
      "Requested number of nodes.\n",
      "\n",
      "NCPUS\n",
      "The number of CPUs used in a job.\n",
      "\n",
      "NNodes\n",
      "The number of nodes used in a job.\n",
      "\n",
      "User\n",
      "The username of the person who ran the job.\n",
      "\n",
      "\n",
      "\n",
      "A full list of variables for the --format flag can be\n",
      "found with the --helpformat flag or by visiting the slurm page on\n",
      "sacct.\n",
      "\n",
      "\n",
      "Debugging failed Jobs\n",
      "If one of your jobs has failed and you need to debug why this has occured you should consult the\n",
      "job records held by the scheduler with the sacct referenced above as well as the generated\n",
      "job logs.\n",
      "These output and error log files will be generated in the job working directory with the job name or\n",
      "output log file name as of the form slurm-$SLURM_JOB_ID.out where $SLURM_JOB_ID is the scheduler provided job id.\n",
      "Looking at these logs should indicate the source of any issues.\n",
      "sacct will also give a job’s state and ExitCode field with each job.\n",
      "The ExitCode is the return value of the exiting program/script. It can be a user defined value if the\n",
      "job is finished with a call to ‘exit(number)’. Any non-zero exit code will be assumed to be a job failure\n",
      "and will result in a Job State of FAILED with a Reason of “NonZeroExitCode”.\n",
      "The job logs may also include a “derived exit code” field. This is set to the value of the highest exit code returned by\n",
      "all of the job’s steps (srun invocations).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Cluster job resource limits\n",
      "While the Sheffield clusters have very large amounts of resources to use for your jobs there\n",
      "are limits applied in order for the schedulers to function.  The limits below apply to the default\n",
      "free queues. Other queues may have different settings.\n",
      "\n",
      "Warning\n",
      "You must ensure that your jobs do not attempt to exceed these limits as the schedulers are not\n",
      "forgiving and will summarily kill any job which exceeds the requested limits without warning.\n",
      "\n",
      "\n",
      "CPU Limits\n",
      "\n",
      "Warning\n",
      "Please note that for either cluster the larger the number of cores you request in an interactive job the more\n",
      "likely the request is to fail as the requested resource is not immediately available.\n",
      "\n",
      "\n",
      "CPU Allocation Limits Table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scheduler Type\n",
      "No. CPU Cores Available   Interactive Job  (Default/ Min / Max )\n",
      "No. CPU Cores Available   Batch Job  (Default/ Min / Max )\n",
      "Submission Argument\n",
      "\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "1 / 1 /  ~11264  (MPI), 64 (SMP)\n",
      "1 / 1 /  ~11264  (MPI), 64 (SMP)\n",
      "-c <nn>\n",
      "\n",
      "SLURM (Bessemer)\n",
      "1 / 1 / 40\n",
      "1 / 1 / 40\n",
      "-c <nn>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time Limits\n",
      "\n",
      "Time Allocation Limits Table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scheduler Type\n",
      "Interactive Job  (Default / Max)\n",
      "Batch Job  (Default / Max)\n",
      "Submission Argument\n",
      "\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "8 / 8 hrs\n",
      "8 / 96 hrs\n",
      "--time=<days-hh:mm:ss>\n",
      "\n",
      "SLURM (Bessemer)\n",
      "8 / 8 hrs\n",
      "8 / 168 hrs\n",
      "--time=<days-hh:mm:ss>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Memory Limits\n",
      "\n",
      "Memory Allocation Limits Table\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "Cross node MPI execution enabled\n",
      "SLURM (Bessemer)\n",
      "Single node execution only\n",
      "\n",
      "\n",
      "\n",
      "Default Job Memory Request\n",
      "4016 MB\n",
      "2 GB\n",
      "\n",
      "Standard Nodes\n",
      "251 MB\n",
      "192 GB\n",
      "\n",
      "Large RAM Nodes\n",
      "1007 GB\n",
      "N/A\n",
      "\n",
      "Very Large RAM Nodes\n",
      "2014 GB\n",
      "N/A\n",
      "\n",
      "Interactive Job\n",
      "Maximum Possible Request\n",
      "251 GB\n",
      "192 GB\n",
      "\n",
      "Batch Job (SMP)\n",
      "Maximum Regular Node Request\n",
      "251 GB\n",
      "192 GB\n",
      "\n",
      "Maximum Possible Request\n",
      "2014 GB\n",
      "192 GB\n",
      "\n",
      "Batch Job (MPI)\n",
      "Maximum Possible Request\n",
      "~74404 GB\n",
      "192 GB\n",
      "\n",
      "Submission Argument on a per node (job) basis\n",
      "–mem=<nn>\n",
      "–mem=<nn>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Advanced / Automated job submission and management\n",
      "Further information on advanced or automated job submission and management can be found on our dedicated\n",
      "pages: Advanced Job Submission and Control and  Advanced Job Profiling and Analysis.\n",
      "\n",
      "\n",
      "Reference information and further resources\n",
      "Quick reference information for the SLURM scheduler used on both the Stanage and Bessemer clusters can be\n",
      "found in the Scheduler Reference Info section.\n",
      "An SGE to SLURM conversion guide is provided in the Quick Reference section.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/advanced_job_submission_and_control.html'}\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/modules.html'}\n",
      "\n",
      "\n",
      "\n",
      "Activating software using Environment Modules\n",
      "\n",
      "Note\n",
      "Additional detailed information on the Environment Modules software can be found on the project’s site.\n",
      "\n",
      "\n",
      "Overview and rationale\n",
      "‘Environment Modules’ are the mechanism by which much of the software is made available to the users of our clusters (Stanage and Bessemer).\n",
      "To make a particular piece of software available a user will load a module e.g.\n",
      "on Stanage you can load a particular version of the OpenFOAM application (version 22.06, built with a particular compiler, BLAS library and MPI implementation (collectively the foss-2022a toolchain)) with:\n",
      "module load OpenFOAM/v2206-foss-2022a\n",
      "\n",
      "\n",
      "This command manipulates environment variables to make this piece of software available.\n",
      "If you then want to switch to using a different version of scotch (should another be installed on the cluster you are using) then you can run:\n",
      "module unload OpenFOAM/v2206-foss-2022a\n",
      "\n",
      "\n",
      "Then load the other version.\n",
      "You may wonder why modules are necessary: why not just install packages provided by the vendor of the operating system installed on the cluster?\n",
      "In shared high-performance computing environments such as our clusters:\n",
      "\n",
      "Users typically want control over the version of applications that is used (e.g. to give greater confidence that results of numerical simulations can be reproduced);\n",
      "Users may want to use applications built using compiler X rather than compiler Y as compiler X might generate faster code and/or more accurate numerical results in certain situations;\n",
      "Users may want a version of an application built with support for particular parallelisation mechanisms such as MPI for distributing work within and between machines, OpenMP for distributing work between CPU cores or CUDA for parallelisation on GPUs);\n",
      "Users may want an application built with support for a particular library.\n",
      "\n",
      "There is therefore a need to maintain multiple versions of the same applications on our clusters.\n",
      "Module files allow users to select and use the versions they need for their research.\n",
      "If you switch to using a cluster other than Stanage or Bessemer then you will likely find that environment modules are used there too.\n",
      "Modules are not the only way of managing software on clusters: increasingly common approaches include:\n",
      "\n",
      "StanageBessemer\n",
      "The Conda package manager (Python-centric but can manage software written in any language);\n",
      "Apptainer/Singularity, a means for deploying software in containers (similar to Docker).\n",
      "\n",
      "\n",
      "The Conda package manager (Python-centric but can manage software written in any language);\n",
      "Apptainer/Singularity, a means for deploying software in containers (similar to Docker).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Basic guide\n",
      "You can list all (loaded and unloaded) modules on our clusters using:\n",
      "module avail\n",
      "\n",
      "\n",
      "You can then load a module using e.g.:\n",
      "module load GEOS/3.9.1-GCC-11.2.0\n",
      "\n",
      "\n",
      "\n",
      "Note\n",
      "Modules are not available on Bessemer’s login nodes. You must start an interactive job on a worker node using srun (see Job Submission and Control) before any of the following commands will work.\n",
      "\n",
      "You can then load further modules e.g.:\n",
      "module load PROJ/8.1.0-GCCcore-11.2.0\n",
      "\n",
      "\n",
      "Confirm which modules you have loaded using:\n",
      "module list\n",
      "\n",
      "\n",
      "If you want to stop using a module (by undoing the changes that loading that module made to your environment):\n",
      "module unload  PROJ/8.1.0-GCCcore-11.2.0\n",
      "\n",
      "\n",
      "Or to unload all loaded modules:\n",
      "module purge\n",
      "\n",
      "\n",
      "To learn more about what software is available on the system and discover the names of module files, you can view the online documentation for\n",
      "\n",
      "Software on Stanage\n",
      "Software on Bessemer\n",
      "\n",
      "The name of a Module should tell you:\n",
      "\n",
      "The type of software (application, library, development tool (e.g. compiler), parallel computing software);\n",
      "The name and version of the software;\n",
      "The name and version of compiler that the software was built using (if applicable; not all installed software was installed from source);\n",
      "The name and version of used libraries that distinguish the different installs of a given piece of software (e.g. the version of OpenMPI an application was built with).\n",
      "\n",
      "Some other things to be aware of:\n",
      "\n",
      "You can load and unload modules in both interactive and batch jobs;\n",
      "Modules may themselves load other modules.  If this is the case for a given module then it is typically noted in our documentation for the corresponding software;\n",
      "Available applications and application versions may differ between our clusters;\n",
      "The order in which you load modules may be significant (e.g. if module A sets SOME_ENV_VAR=apple and module B sets SOME_ENV_VAR=pear);\n",
      "Related module files e.g. multiple versions of the same application typically cannot be loaded concurrently.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Searching for Modules\n",
      "\n",
      "StanageBessemerYou can search for a module using:\n",
      "module -t --redirect avail |& grep -i somename\n",
      "\n",
      "\n",
      "Where you replace somename with the string you wish to search for.\n",
      "You may wish to setup a bash alias in your $HOME/.bashrc file with this as a short cut e.g. :\n",
      "alias modulefind=\"module -t --redirect avail |& grep -i\"\n",
      "\n",
      "\n",
      "After sourcing $HOME/.bashrc this command can then be called like so:\n",
      "$ source $HOME/.bashrc\n",
      "$ modulefind fftw\n",
      "FFTW.MPI/\n",
      "FFTW.MPI/3.3.10-gompi-2022a\n",
      "FFTW.MPI/3.3.10-gompi-2022b\n",
      "FFTW/\n",
      "FFTW/3.3.8-gompi-2019b\n",
      "FFTW/3.3.8-gompi-2020a\n",
      "FFTW/3.3.8-gompi-2020b\n",
      "FFTW/3.3.10-GCC-11.3.0\n",
      "FFTW/3.3.10-GCC-12.2.0\n",
      "imkl-FFTW/\n",
      "imkl-FFTW/2021.4.0-iimpi-2021b\n",
      "imkl-FFTW/2022.1.0-iimpi-2022a\n",
      "imkl-FFTW/2022.2.1-iimpi-2022b\n",
      "\n",
      "\n",
      "Another option is to use:\n",
      "module spider somename\n",
      "\n",
      "\n",
      "You can search for a module using:\n",
      "module avail |& grep -i somename\n",
      "\n",
      "\n",
      "Where you replace somename with the string you wish to search for.\n",
      "You may wish to setup a bash alias in your $HOME/.bashrc file with this as a short cut e.g. :\n",
      "alias modulefind=\"module avail |& grep -i\"\n",
      "\n",
      "\n",
      "After sourcing $HOME/.bashrc this command can then be called like so:\n",
      "$ source $HOME/.bashrc\n",
      "$ modulefind intel\n",
      "CFITSIO/3.45-intel-2018b\n",
      "DL_POLY_4_PLUMED_INTEG/5.0.0-intel-2020b\n",
      "FDS/6.7.5-intel-2020a\n",
      "FFTW/3.3.8-intel-2019a\n",
      "intel/2018b\n",
      "intel/2019a\n",
      "intel/2019b\n",
      "intel/2020a\n",
      "intel/2020b\n",
      "PLUMED/2.6.2-intel-2020b\n",
      "SciPy-bundle/2020.11-intel-2020b\n",
      "VASP/5.4.1-intel-2019b\n",
      "VASP/5.4.4-intel-2019b\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Behind the scenes\n",
      "Let’s look at what happens when you load an environment module.\n",
      "If we inspect the contents of a module file we see something like:\n",
      "$ module show dev/NAG/6.1\n",
      "-------------------------------------------------------------------\n",
      "/usr/local/modulefiles/dev/NAG/6.1:\n",
      "\n",
      "module-whatis   Makes the NAG Fortran Compiler v6.1 available\n",
      "conflict        dev/NAG\n",
      "prepend-path    PATH /usr/local/packages/dev/NAG/6.1/bin\n",
      "prepend-path    MANPATH /usr/local/packages/dev/NAG/6.1/man\n",
      "setenv          NAG_KUSARI_FILE /usr/local/packages/dev/NAG/license.lic\n",
      "\n",
      "\n",
      "Here we see:\n",
      "\n",
      "The full path to the file that contains the definition of this module;\n",
      "A line briefly describing the purpose of the module (which could have been viewed separately using module whatis dev/NAG/6.1);\n",
      "An instruction not to load any other module files that start with dev/NAG as they will cause a conflict;\n",
      "A directory is prepended to the standard PATH variable: this ensures that executables relating to dev/NAG/6.1 are preferentially used unrelated executables in PATH directories that share the same filenames.  Note that this directory is specific to this version (6.1) of the application we want to use;\n",
      "A directory is prepended to the standard MANPATH variable to ensure that the documentation (man pages) that the vendor bundled with the application can be found;\n",
      "An application-specific environment variable, NAG_KUSARI_FILE, is set (here to ensure that the application can find a license file).\n",
      "\n",
      "If you run the ‘env’ command before and after loading a module you can see the effect of these changes.\n",
      "\n",
      "\n",
      "\n",
      "Convenient ways to set up your environment for different projects\n",
      "If you regularly need to activate multiple modules whilst working on a given project\n",
      "it may be tempting to add the necessary module load commands to a shell startup script\n",
      "(e.g. the .bashrc script in your home directory).\n",
      "However, this is a bad idea for several reasons:\n",
      "\n",
      "Over time you will forget what is in your .bashrc and may forget that your workflow is dependent on modules loaded by the script;\n",
      "Your .bashrc script may not be managed using version control (e.g. Git) or,\n",
      "if it is, it is unlikely to be in the same repository as your project scripts/code;\n",
      "If someone asks you in three months’ time what version of an application you used to run a simulation will you be able to tell them?\n",
      "\n",
      "A better approach is to create a module-loading script inside the directory containing your project’s other scripts\n",
      "then source (run) this script.\n",
      "For example, you could have project scripts stored in a directory called /home/te1st/proj1.\n",
      "You could create a script in that directory called setup_env.sh containing:\n",
      "module load compilers/pgi/13.1\n",
      "module load mpi/pgi/openmpi/1.6.4\n",
      "\n",
      "\n",
      "Then if you want to load these modules in an interactive session or in a batch job you could run:\n",
      "source /home/te1st/proj1/setup_env.sh\n",
      "\n",
      "\n",
      "If you want to run the job on Stanage and Bessemer (which provide different software / module files)\n",
      "you could adapt your script to load different modules depending on which cluster you are using:\n",
      "if [[ \"$HOSTNAME\" == *\"stanage\"* ]]; then\n",
      "    # On Stanage:\n",
      "    module load some/module\n",
      "    module load another/module\n",
      "elif [[ \"$HOSTNAME\" == *\"bessemer\"* ]]; then\n",
      "    # On Bessemer:\n",
      "    hostname=\"bessemer\"\n",
      "    module load different/module\n",
      "fi\n",
      "\n",
      "\n",
      "Managing your environment this way is more likely to result in reproducible research,\n",
      "particularly if changes to the content of /home/te1st/proj1 are tracked using Git or another version control tool\n",
      "\n",
      "\n",
      "\n",
      "Managing your own module files\n",
      "Modules are a great way of loading/unloading software installed in non-standard places.\n",
      "You may therefore want to use them to manage software installed in\n",
      "\n",
      "your home directory\n",
      "a directory shared by your research group\n",
      "\n",
      "If you want your own Modules, you typically need to create a hierarchy of directories and files.\n",
      "Within a base directory the relative path to a given module file determines the name you need to use to load it.\n",
      "Access the directories stored in the variable $MODULEPATH to:\n",
      "\n",
      "see the files that provide all cluster-wide modules and\n",
      "get an understanding of the (Tcl) syntax and structure of module files.\n",
      "\n",
      "A tutorial on how to write module files is not provided here (but may be in future).\n",
      "Once you’ve created a set of module files within a directory you can make the module system aware of them by running:\n",
      "module use /the/path/to/my/modules\n",
      "\n",
      "\n",
      "The next time you run module avail you will see that your modules are listed alongside the cluster-wide modules.\n",
      "If you no longer want to to have access to your own module files then you can run:\n",
      "module unuse /the/path/to/my/modules\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Compiling software dependent on modules\n",
      "In most cases, if you are compiling software with dependencies on modules the only actions you need to take are to load the required modules, run any ./configure or CMake steps\n",
      "and then run the make, make check (if available) and make install commands to build, check and install the software.\n",
      "Once the software is installed, each time you use the software you must first load the modules used to compile it. This is necessary to make the required libraries and other files used during the compilation available to the program.\n",
      "For more detailed information on the software installation process, please see: Installing software to the clusters.\n",
      "You will have to construct/edit your own customised makefile which may have to reference specific libraries and paths if:\n",
      "\n",
      "There are no preconfiguration steps available to generate a suitable makefile based on the current shell environment after loading modules.\n",
      "An example makefile for editing is provided.\n",
      "No makefile is provided.\n",
      "\n",
      "In this case, you can use the module show modulename command to show how the module file for your loaded software module/s are interacting with your shell environment to populate the $PATH, $LD_LIBRARY_PATH\n",
      "and other environment variables.\n",
      "You can then navigate to any directories of interest or use the find or grep commands to search them as required.\n",
      "\n",
      "\n",
      "\n",
      "Module Command Reference\n",
      "Here is a list of the most useful module commands. For full details, type man module at the command prompt on one of the clusters.\n",
      "\n",
      "StanageBessemer\n",
      "module list – lists currently loaded modules\n",
      "module avail – lists all available modules\n",
      "module load modulename – loads module modulename\n",
      "module unload modulename – unloads module modulename\n",
      "module switch oldmodulename newmodulename – switches between two modules\n",
      "module show modulename - Shows how loading modulename will affect your environment\n",
      "module purge – unload all modules\n",
      "module help modulename – may show longer description of the module if present in the modulefile\n",
      "man module – detailed explanation of the above commands and others\n",
      "ml --help – outlines module shorthand commands\n",
      "\n",
      "\n",
      "module list – lists currently loaded modules\n",
      "module avail – lists all available modules\n",
      "module load modulename – loads module modulename\n",
      "module unload modulename – unloads module modulename\n",
      "module switch oldmodulename newmodulename – switches between two modules\n",
      "module show modulename - Shows how loading modulename will affect your environment\n",
      "module purge – unload all modules\n",
      "module help modulename – may show longer description of the module if present in the modulefile\n",
      "man module – detailed explanation of the above commands and others\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/Choosing-appropriate-resources.html'}\n",
      "\n",
      "\n",
      "\n",
      "Choosing appropriate compute resources\n",
      "\n",
      "Introduction\n",
      "Choosing appropriate resources for your jobs is essential to ensuring your jobs will be scheduled as quickly as possible while wasting as little resources as possible.\n",
      "The key resources you need to optimise for are:\n",
      "\n",
      "Cluster-choice.\n",
      "Time-allocation.\n",
      "Cores-allocation.\n",
      "Memory-allocation.\n",
      "Filestore-limits.\n",
      "\n",
      "It is important to be aware that the resource requests that you make are not flexible: if your job exceeds what you have requested for it the scheduler will terminate your job\n",
      "abruptly and without any warning. This means that it is safest to over estimate your job’s requirements if they cannot be accurately and precisely known in advance.\n",
      "This does not mean that you can set extremely large values for these resource requests for several reasons, the most important being:\n",
      "\n",
      "Large allocations will take longer to queue and start.\n",
      "Allocations larger than the scheduler can ever satisfy with the available resources will never start.\n",
      "\n",
      "\n",
      "\n",
      "Cluster choice\n",
      "We have two cluster choices listed below for you to choose from:\n",
      "\n",
      "Stanage (Our newest and most powerful yet, launched in March 2023).\n",
      "Bessemer (Launched in 2018).\n",
      "\n",
      "It is also important to note that the Sheffield HPC clusters have been designed to fulfil different purposes. Stanage is for the most part a `capability` cluster designed to\n",
      "run larger compute jobs that will use multiple nodes. Bessemer is a `capacity` cluster designed to run smaller compute jobs which will fit on a single node.\n",
      "You should prioritize putting smaller core count jobs onto Bessemer and massively parallel jobs onto Stanage (while utilizing a form of MPI).\n",
      "In addition, Stanage has newer CPUs and GPUs with more modern features. Both clusters share similar file storage areas, each which are tuned for certain workloads.\n",
      "More cluster specific information: Stanage Specifications and Bessemer specifications .\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Time Allocation Limits\n",
      "\n",
      "Time Allocation Limits Table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scheduler Type\n",
      "Interactive Job  (Default / Max)\n",
      "Batch Job  (Default / Max)\n",
      "Submission Argument\n",
      "\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "8 / 8 hrs\n",
      "8 / 96 hrs\n",
      "--time=<days-hh:mm:ss>\n",
      "\n",
      "SLURM (Bessemer)\n",
      "8 / 8 hrs\n",
      "8 / 168 hrs\n",
      "--time=<days-hh:mm:ss>\n",
      "\n",
      "\n",
      "\n",
      "The time allocation limits will differ between job types and by cluster. A summary of these differences can be seen above. Time requirements are highly dependent on\n",
      "how many CPU cores your job is using - using more cores may significantly decrease the amount of time the job spends running, depending on how optimally the software\n",
      "you are using supports parallelisation. Further details on CPU cores selection can be found in the CPU cores allocation section.\n",
      "\n",
      "Determining time requirements using timing commands in your script\n",
      "A way of deducing the “wall clock” time used by a job is to use the date command within the batch script file. The date command is part of the Linux operating\n",
      "system. Here is an example:\n",
      "#SBATCH --time=00:10:00\n",
      "date\n",
      "my_program < my_input\n",
      "date\n",
      "\n",
      "\n",
      "When the above script is submitted (via sbatch), the job output file will contain the date and time at each invocation of the date command. You can then calculate the\n",
      "difference between these date/times to determine the actual time taken.\n",
      "\n",
      "\n",
      "Determining time used by your jobs\n",
      "The time used by a job is typical quantified into 2 values by the scheduler:\n",
      "\n",
      "the “wallclock” (the time your job took if measured by a clock on your wall)\n",
      "the consumed “CPU time” (a number of seconds of compute, derived directly from the amount of CPU time used on all cores of a job).\n",
      "\n",
      "How to determine these values can be seen below using the seff command as below:\n",
      "The seff script can be used as follows with the job’s ID to give summary of important job info including the wallclock time:\n",
      "$ seff 64626\n",
      "Job ID: 64626\n",
      "Cluster: a_cluster\n",
      "User/Group: a_user/a_group\n",
      "State: COMPLETED (exit code 0)\n",
      "Nodes: 1\n",
      "Cores per node: 2\n",
      "CPU Utilized: 00:02:37\n",
      "CPU Efficiency: 35.68% of 00:07:20 core-walltime\n",
      "Job Wall-clock time: 00:03:40\n",
      "Memory Utilized: 137.64 MB (estimated maximum)\n",
      "Memory Efficiency: 1.71% of 7.84 GB (3.92 GB/core)\n",
      "\n",
      "\n",
      "Here we can see the wallclock was 03:40 (220s) and the consumed CPU time was 02:37 (157s). As this job requested 2 cores (1 node * 2 cores), we can also see there was a maximum core-walltime of 07:20 (440s) available.\n",
      "The CPU Efficiency follows as (157/440)*100=35.68%.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "CPU Allocation Limits\n",
      "\n",
      "CPU Allocation Limits Table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Scheduler Type\n",
      "No. CPU Cores Available   Interactive Job  (Default/ Min / Max )\n",
      "No. CPU Cores Available   Batch Job  (Default/ Min / Max )\n",
      "Submission Argument\n",
      "\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "1 / 1 /  ~11264  (MPI), 64 (SMP)\n",
      "1 / 1 /  ~11264  (MPI), 64 (SMP)\n",
      "-c <nn>\n",
      "\n",
      "SLURM (Bessemer)\n",
      "1 / 1 / 40\n",
      "1 / 1 / 40\n",
      "-c <nn>\n",
      "\n",
      "\n",
      "\n",
      "The CPU allocation limits will differ between job types and by cluster - a summary of these differences can be seen above. It is important to note that SLURM and SGE will request CPU on a different basis as detailed above.\n",
      "\n",
      "Determining CPU requirements:\n",
      "In order to determine your CPU requirements, you should investigate if your program / job supports parallel execution. If the program only supports serial processing, then you can only use 1 CPU core and should be using Bessemer (faster CPUs) to do so.\n",
      "If your job / program supports multiple cores, you need to assess whether it supports SMP (symmetric multiprocessing) where you can only use CPUs on 1 node or MPI (message passing interface) where you can access as many nodes, CPUs and cores as are available.\n",
      "For single-node jobs: you can use a maximum of 64 cores on Stanage and 40 cores on Bessemer.\n",
      "For multiple-node MPI-type parallel processing jobs: these can run on Stanage and although you can access as many cores as are available you must consider how long a job will take to queue waiting for resources compared the the decrease in time for the job to complete computation.\n",
      "Single-node MPI-type parallel jobs can run on Stanage and Bessemer.\n",
      "For both parallel processing methods you should run several test jobs using the tips from the Time allocation section with various numbers of cores to assess what factor of speedup/slowdown is attained for queuing and computation / the total time for job completion.\n",
      "Remember, the larger your request, the longer it will take for the resources to become available and the time taken to queue is highly dependent on other cluster jobs.\n",
      "Some additional important considerations to make are:\n",
      "\n",
      "Amdahl’s law - an increase in cores or computational power will not scale in a perfectly linear manner. Using 2 cores will not be twice as fast as a single core - and the proportional time reduction from using more cores will decrease with larger core counts.\n",
      "Job workload optimisation is highly dependent on the workload type - workloads can be CPU, memory bandwidth or IO (reading and writing to disk) limited - detailed exploration and profiling of workloads is beyond the scope of this guide.\n",
      "Trying a smaller job (or preferably a set of smaller jobs of different sizes) will allow you to extrapolate likely resource requirements but you must remain aware of the limitations as stated above.\n",
      "\n",
      "\n",
      "\n",
      "Determining job CPU efficiencies\n",
      "When quantifying the CPU usage efficiency two values are important:\n",
      "\n",
      "the “wallclock” (the time your job took if measured by a clock on your wall)\n",
      "the consumed “CPU time” (a number of seconds of compute, derived directly from the amount of CPU time used on all cores of a job).\n",
      "\n",
      "To optimise your CPU requests you can investigate how efficiently your job is making use of your requested cores with the seff command:\n",
      "The seff script can be used as follows with the job’s ID to give summary of important job info including the wallclock time:\n",
      "$ seff 64626\n",
      "Job ID: 64626\n",
      "Cluster: a_cluster\n",
      "User/Group: a_user/a_group\n",
      "State: COMPLETED (exit code 0)\n",
      "Nodes: 1\n",
      "Cores per node: 2\n",
      "CPU Utilized: 00:02:37\n",
      "CPU Efficiency: 35.68% of 00:07:20 core-walltime\n",
      "Job Wall-clock time: 00:03:40\n",
      "Memory Utilized: 137.64 MB (estimated maximum)\n",
      "Memory Efficiency: 1.71% of 7.84 GB (3.92 GB/core)\n",
      "\n",
      "\n",
      "Here we can see the wallclock was 03:40 (220s) and the consumed CPU time was 02:37 (157s). As this job requested 2 cores (1 node * 2 cores), we can also see there was a maximum core-walltime of 07:20 (440s) available.\n",
      "The CPU Efficiency follows as (157/440)*100=35.68%.\n",
      "The ideal value for CPU efficiency is 100%.\n",
      "If a value of 100/n requested cores is observed, you are likely to be using a single threaded program (which cannot benefit from multiple cores) or a multithreaded program incorrectly configured to use the multiple cores requested.\n",
      "In general, you should request a single core for single threaded programs and ensure multicore programs are correctly configured with as few cores as possible requested to shorten your queue time.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Memory Allocation Limits\n",
      "\n",
      "Memory Allocation Limits Table\n",
      "\n",
      "\n",
      "SLURM (Stanage)\n",
      "Cross node MPI execution enabled\n",
      "SLURM (Bessemer)\n",
      "Single node execution only\n",
      "\n",
      "\n",
      "\n",
      "Default Job Memory Request\n",
      "4016 MB\n",
      "2 GB\n",
      "\n",
      "Standard Nodes\n",
      "251 MB\n",
      "192 GB\n",
      "\n",
      "Large RAM Nodes\n",
      "1007 GB\n",
      "N/A\n",
      "\n",
      "Very Large RAM Nodes\n",
      "2014 GB\n",
      "N/A\n",
      "\n",
      "Interactive Job\n",
      "Maximum Possible Request\n",
      "251 GB\n",
      "192 GB\n",
      "\n",
      "Batch Job (SMP)\n",
      "Maximum Regular Node Request\n",
      "251 GB\n",
      "192 GB\n",
      "\n",
      "Maximum Possible Request\n",
      "2014 GB\n",
      "192 GB\n",
      "\n",
      "Batch Job (MPI)\n",
      "Maximum Possible Request\n",
      "~74404 GB\n",
      "192 GB\n",
      "\n",
      "Submission Argument on a per node (job) basis\n",
      "–mem=<nn>\n",
      "–mem=<nn>\n",
      "\n",
      "\n",
      "\n",
      "The memory allocation limits will differ between job types and by cluster - a summary of these differences can be seen above. It is important to note that SLURM and SGE will request memory on a different basis as detailed above.\n",
      "\n",
      "Determining memory requirements:\n",
      "By using the emailing parameters of the sbatch command:\n",
      "Submit your job with sbatch by specifying very generous memory and time requirements to ensure that it runs to completion” and also using the --mail-user= and --mail-type=ALL  parameters to receive an email-report. The mail message will list the maximum memory usage ( maxvmem / MaxVMSize  ) as well as the wallclock time used by the job.\n",
      "#SBATCH --mem=8G\n",
      "#SBATCH --time=01:00:00\n",
      "#SBATCH --mail-user=joe.blogs@sheffield.ac.uk\n",
      "#SBATCH --mail-type=ALL\n",
      "myprog < mydata.txt > myresults.txt\n",
      "\n",
      "\n",
      "When the job completes, you will receive an email reporting the memory and time usage figures.\n",
      "\n",
      "By using the seff/sstat/sacct command:\n",
      "For a quick summary, the seff command can be used as follows with the job’s ID to give summary of important job info including the memory usage / efficiency:\n",
      "$ seff 64626\n",
      "Job ID: 64626\n",
      "Cluster: stanage.alces.network\n",
      "User/Group: a_user/clusterusers\n",
      "State: COMPLETED (exit code 0)\n",
      "Nodes: 2\n",
      "Cores per node: 1\n",
      "CPU Utilized: 00:02:37\n",
      "CPU Efficiency: 35.68% of 00:07:20 core-walltime\n",
      "Job Wall-clock time: 00:03:40\n",
      "Memory Utilized: 137.64 MB (estimated maximum)\n",
      "Memory Efficiency: 1.71% of 7.84 GB (3.92 GB/core)\n",
      "\n",
      "\n",
      "For more specific info, you can use the sacct / sstat commands:\n",
      "While a job is still running find out its job id by:\n",
      "sacct\n",
      "\n",
      "\n",
      "And check its current usage of memory by:\n",
      "sstat job_id --format='JobID,MaxVMSize,MaxRSS'\n",
      "\n",
      "\n",
      "If your job has already finished you can list the memory usage with sacct:\n",
      "sacct --format='JobID,Elapsed,MaxVMSize,MaxRSS'\n",
      "\n",
      "\n",
      "It is the MaxVMSize / MaxRSS figures that you will need to use to determine the --mem= parameter for your next job.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Filestore Limits / file store performance characteristics\n",
      "Every HPC user is allocated a file-storage area of their own. Please read the section on  Filestores for the clusters for further information. Any attempt to exceed a file store quota during the execution of a job can have disastrous consequences. This is because any program or package writing into files will produce a fatal error and stop if the filestore limit happens to be exceeded during that operation.\n",
      "Filestore limits are not associated with jobs and can not be specified while submitting a job. Users must make sure that there is sufficient spare space in their filestore areas before submitting any job that is going to produce large amounts of output. It may be necessary for users to use multiple filestores during longer projects or even within one job.\n",
      "The quota command can be used to check your current filestore allocation and usage.\n",
      "Each filestore has the relevant detail and performance characteristics listed within the section on Filestores, this will indicate where your program is best suited to run from.\n",
      "\n",
      "\n",
      "Determining how much storage space your jobs will need:\n",
      "One method to determine how much space your jobs are likely to consume is to run an example job within a specific directory saving the output within.\n",
      "Once the run has completed you can determine the amount of storage taken by the job by running:\n",
      "du -sh my_directory_name\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Special limits and alternative queues\n",
      "If you have paid for a reservation, your research group or department has purchased additional resources there may be other accounts and partitions you can specify which will override normal limits imposed on the cluster jobs.\n",
      "\n",
      "Specific group nodes on Bessemer\n",
      "\n",
      "If you have access to additional queues / partitions and want to know their limitations you can using the following commands to explore this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Listing Queues\n",
      "You can list the queues with:\n",
      "sinfo -a\n",
      "\n",
      "\n",
      "You can list more information by formatting the output, e.g.:\n",
      "sinfo -o \"%P %l %c %D \"  # PARTITION TIMELIMIT CPUS NODES\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/installing-software.html'}\n",
      "\n",
      "\n",
      "\n",
      "Installing software to the clusters\n",
      "\n",
      "Contents\n",
      "\n",
      "\n",
      "General background prequisites\n",
      "\n",
      "What is source code?\n",
      "What is a compiler or compiling?\n",
      "What are binaries?\n",
      "What about software dependencies?\n",
      "What is a Linux shell?\n",
      "What are environment variables?\n",
      "\n",
      "\n",
      "Installing software from binaries\n",
      "\n",
      "Downloading your binaries\n",
      "Unpacking your binaries\n",
      "Making your binaries available in the shell\n",
      "\n",
      "\n",
      "Installing software by compiling from source\n",
      "\n",
      "Downloading the source code\n",
      "Compiling your source code into binaries\n",
      "Making your compiled binaries available in the shell\n",
      "\n",
      "\n",
      "Making installed software available to execute\n",
      "\n",
      "The .bashrc file and its purpose\n",
      "Making software available via the .bashrc file\n",
      "Environment ‘Modules’ and their purpose\n",
      "Making software available via a custom module file\n",
      "\n",
      "\n",
      "Why should I install from source?\n",
      "What alternative methods exist?\n",
      "\n",
      "\n",
      "\n",
      "As Stanage and Bessemer are general purpose HPC clusters,\n",
      "we provide and maintain only the most essential and most popular applications on them.\n",
      "We are aware of our users’ need to run applications that are specific to their own subject\n",
      "areas of research and as such we permit the installation of software within users’ personal directories\n",
      "and special shared areas on the clusters for public use.\n",
      "This option should be seen as a service without support as we will expect such users to be able to\n",
      "tackle the problems encountered during installations on their own. We will however help make such\n",
      "software available to other Stanage and Bessemer users by copying/installing scripts to shared locations.\n",
      "\n",
      "Policy on user-installed software on University of Sheffield HPC systems\n",
      "\n",
      "\n",
      "\n",
      "Users should endeavour to download source code or software binaries\n",
      "produced by trusted developers/vendors and\n",
      "acquired from trusted repositories/locations.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Users should keep software up to date where reproducibility is not a concern.\n",
      "Users should remove any software that is definitely no longer needed.\n",
      "\n",
      "\n",
      "To discuss and get support regarding these requirements\n",
      "please contact research-it@sheffield.ac.uk\n",
      "\n",
      "\n",
      "General background prequisites\n",
      "\n",
      "Tip\n",
      "If you are not familiar with basic computer architecture we highly recommend reading our\n",
      "General Computer Architecture Quick Start page\n",
      "before continuing.\n",
      "\n",
      "\n",
      "What is source code?\n",
      "Source code is the collection of code written in a human readable programming language for a given\n",
      "software package. Source code is transformed by a compiler into machine code that can be\n",
      "executed by a computer.\n",
      "\n",
      "\n",
      "\n",
      "What is a compiler or compiling?\n",
      "The shortest description of what a compiler is / what compiling is that it is a process that\n",
      "takes human made source code and turns it into machine code that will run on a computer.\n",
      "Machine code has to be specific to a given processor’s architecture and the instruction sets it supports\n",
      "(aka, instructions/operations that a CPU can do) which is why you may need to compile your\n",
      "code for a specific instruction set (different processor manufacturers design different processors\n",
      "sometimes with different instruction sets) .\n",
      "For example, you are probably aware that mobile phones use ARM processors not Intel or AMD processors\n",
      "that you will typically find in a desktop or laptop computer. This difference in processors and their\n",
      "instruction sets is one of the reasons why applications that run on phones cannot typically\n",
      "run on desktop computers.\n",
      "Within research, you may find certain clusters using different processor architectures which have been\n",
      "designed for optimal performance at certain tasks using different instruction sets.\n",
      "e.g. Power 9 architecture on the BEDE cluster.\n",
      "This also means that to run software on these machines with different architecures you may need to\n",
      "recompile the software from source code if no binaries for that architecture are provided!\n",
      "You may be wondering why you need to compile some software but not others, this is due to the\n",
      "differences between compiled and interpreted languages\n",
      ", but this falls out of the scope of this page.\n",
      "\n",
      "\n",
      "\n",
      "What are binaries?\n",
      "When referring to software, software binaries, binary installations or binary downloads are\n",
      "software packages supplied to you pre-compiled by the developer for a specific processor /\n",
      "instruction set. This means that if you wish to use a binary software build you must check that you\n",
      "download and install the correct version that matches your machine’s processor / architecture.\n",
      "\n",
      "\n",
      "\n",
      "What about software dependencies?\n",
      "Many software packages have numerous libraries or other software packages on which they are dependant\n",
      "in order to function.\n",
      "This means that the installation of one software package may require multiple packages requiring\n",
      "installation and loading prior or existing software modules provided on the cluster may need to\n",
      "be loaded prior in order for the software to install or function correctly.\n",
      "\n",
      "\n",
      "\n",
      "What is a Linux shell?\n",
      "A shell is a program that takes commands typed from the keyboard and gives them to the computer to run.\n",
      "Historically the shell was the only user interface available on a Unix-like system such as Linux. In the present\n",
      "day, graphical user interfaces (GUIs) are available in addition to interfaces such as the shell.\n",
      "Most Linux operating systems use a program called bash (the Bourne Again SHell, an enhanced version of the original\n",
      "Unix shell program, sh, written by Steve Bourne) as the shell program. There are other shell programs available for\n",
      "Linux systems if desired by a user. Examples include: ash, dash, csh, tcsh, ksh and zsh.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What are environment variables?\n",
      "In Linux based operating systems, environment variables are dynamic named values stored within the\n",
      "system which are used by shells or subshells (your terminal) to facilitate functionality. Simply put,\n",
      "they are variables with a name and value which perform a function in how the operating system and\n",
      "applications work.\n",
      "These variables have a simple format:\n",
      "KEY=value\n",
      "KEY=\"Some other value\"\n",
      "KEY=value1:value2\n",
      "\n",
      "\n",
      "\n",
      "Important\n",
      "\n",
      "The variable names are case sensitive and by convention they are UPPER CASE.\n",
      "If a variable has multiple values they should be separated by a colon :.\n",
      "Variables do not have spaces around the equals = sign.\n",
      "\n",
      "\n",
      "Note that environment variables are variables that are available system-wide and are inherited\n",
      "by all spawned child processes and shells where shell variables are variables that apply only to\n",
      "the current shell instance. Each shell such as bash (the default on the clusters), has its own\n",
      "set of internal shell variables.\n",
      "\n",
      "\n",
      "Listing environment variables\n",
      "\n",
      "env – This command allows you to run another program in a custom environment without modifying\n",
      "the current one. When used without an argument it will print a list of the current environment variables.\n",
      "printenv – This command prints all or the specified environment variables.\n",
      "echo $MYVARIABLE - The command echo when supplied with a variable name prefixed with $ will\n",
      "print that variable. An alternative syntax would be echo ${MYVARIABLE}. Variables can also be\n",
      "utilized in bash scripts in this manner.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Setting environment variables\n",
      "Manually setting environment variables is trivial and can be accomplished with the commands below.\n",
      "\n",
      "set – The command sets or unsets shell variables. When used without an argument it will print a\n",
      "list of all variables including environment and shell variables, and shell functions.\n",
      "unset – The command deletes shell and environment variables.\n",
      "export – The command sets environment variables.\n",
      "\n",
      "\n",
      "Caution\n",
      "Setting or changing environment variables can lead to a corrupted shell environment which can leave you\n",
      "unable to login or run programs. Manually changing values should be avoided in favour of using the\n",
      "modules system.\n",
      "If you find your shell environment is behaving oddly, programs are no longer available and\n",
      "you suspect you may have corrupted your current shell environment by changing environment variables\n",
      "in the terminal you can simply log out and log back in to clear the problem.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How do environment variables relate to installing software?\n",
      "The usage of environment variables is critical to not only installing the software where you desire\n",
      "but also to making those software executables available to use in your shell.\n",
      "A few of the most important variables are listed below with HOME,  USER and LANG variables\n",
      "useful during installlation (e.g. setting directories in which to install) and the PATH and\n",
      "LD_LIBRARY_PATH variables used to add libraries or executables to your shell.\n",
      "\n",
      "The HOME environment variable contains the path of your user’s home directory.\n",
      "The USER environment variable contains the username of your current user.\n",
      "The PATH environment variable is a list of directories where your executables are located,\n",
      "adding a directory to this list makes any of the executables in that directory available\n",
      "from the terminal via their name.\n",
      "The LD_LIBRARY_PATH functions similarly, but is a list of directories where your\n",
      "libraries are located. Adding a directory to this list makes any of the libraries in\n",
      "that directory available to programs.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Installing software from binaries\n",
      "\n",
      "Caution\n",
      "Installing from pre-compiled binaries does not remove the need to supply correctly versioned\n",
      "dependencies (e.g. shared libraries).\n",
      "Using incorrectly versioned dependencies may allow a program to function but this could lead to\n",
      "instability and software errors.\n",
      "\n",
      "\n",
      "Downloading your binaries\n",
      "The first step of completing and installation from binaries on the clusters is to download the binaries.\n",
      "In general there are few methods for downloading your binaries which will be detailed below in the\n",
      "prefered order.\n",
      "\n",
      "\n",
      "1. Downloading binaries for the cluster using Yumdownloader\n",
      "Yumdownloader is an application installed on the cluster which will allow you to download RPM packaged\n",
      "applications directly from the cluster operating system’s repositories.\n",
      "This is the best method as this will natively ensure that you get a version that is not only\n",
      "compatible with the operating system but this will also ensure that the package is downloaded\n",
      "from a trusted location.\n",
      "As an example the following command will download the GNU Make RPM to your local folder indicating\n",
      "where it is downloading the RPM from as well as the full name of the file downloaded.\n",
      "\n",
      "Important\n",
      "GNU Make is already available on our clusters! Any further examples of installing or compiling GNU Make are\n",
      "examples only, you do not need to download or install Make.\n",
      "\n",
      "[user@node004 [stanage] yumpackages]$ yumdownloader make\n",
      "Loaded plugins: fastestmirror, priorities\n",
      "Loading mirror speeds from cached hostfile\n",
      "* epel: ftp.nluug.nl\n",
      "make-3.82-24.el7.x86_64.rpm                                | 421 kB  00:00:00\n",
      "[user@node004 [stanage] yumpackages]$\n",
      "\n",
      "\n",
      "This method will automatically check the package integrity and check it also has valid signatures.\n",
      "\n",
      "\n",
      "\n",
      "2. Downloading binaries from pkgs.org\n",
      "pkgs.org is a website which allows a user to search for and download binary packages\n",
      "for numerous Linux and Unix operating systems. Using this website you will be able to query for CentOS 7\n",
      "x86_64 compatible packages and then download them.\n",
      "\n",
      "Caution\n",
      "It is possible to download and use packages for different versions of CentOS (or RHEL as both\n",
      "operating systems are binary compatible) but this is not recommended and may lead to application\n",
      "instability or errors.\n",
      "\n",
      "Using GNU Make again as an example, the required page can be found by searching as:\n",
      "https://centos.pkgs.org/7/centos-x86_64/make-3.82-24.el7.x86_64.rpm.html\n",
      "Looking at the Download section, the binary package download URL can be seen as:\n",
      "http://mirror.centos.org/centos/7/os/x86_64/Packages/make-3.82-24.el7.x86_64.rpm\n",
      "This RPM can now be downloaded using the wget command on the cluster:\n",
      "[user@node004 [stanage] yumpackages]$ wget http://mirror.centos.org/centos/7/os/x86_64/Packages/make-3.82-24.el7.x86_64.rpm\n",
      "--2021-07-15 12:19:18--  http://mirror.centos.org/centos/7/os/x86_64/Packages/make-3.82-24.el7.x86_64.rpm\n",
      "Resolving mirror.centos.org (mirror.centos.org)... 85.236.43.108, 2604:1380:2001:d00::3\n",
      "Connecting to mirror.centos.org (mirror.centos.org)|85.236.43.108|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 430712 (421K) [application/x-rpm]\n",
      "Saving to: ‘make-3.82-24.el7.x86_64.rpm’\n",
      "\n",
      "100%[==================================================================================================>] 430,712     --.-K/s   in 0.1s\n",
      "\n",
      "2021-07-15 12:19:18 (3.74 MB/s) - ‘make-3.82-24.el7.x86_64.rpm’ saved [430712/430712]\n",
      "\n",
      "\n",
      "Because we have downloaded this manually we should now verify both the package integrity and that the\n",
      "package has been signed as trusted. We can do this with the rpm --checksig command.\n",
      "[user@node004 [stanage] yumpackages]$ rpm --checksig make-3.82-24.el7.x86_64.rpm\n",
      "make-3.82-24.el7.x86_64.rpm: rsa sha1 (md5) pgp md5 OK\n",
      "\n",
      "\n",
      "\n",
      "Hint\n",
      "The pkgs.org website will also show the dependencies of a package in the\n",
      "Requires section. This can be very useful for resolving package / library dependencies.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "3. Downloading binaries from a vendor / package maintainer\n",
      "If you have software from a vendor who does not supply source code or a package maintainer has provided\n",
      "binaries that are not supplied as part of the normal package repositories for the operating system you\n",
      "will typically be supplied by them with a RPM file (package.rpm) or a compressed tarball (package.tar.gz)\n",
      "from their website, via email or similar.\n",
      "You may be able to use the wget command to download this directly to the cluster or may have to\n",
      "transfer this manually using SCP or similar. Once downloaded you should verify the software download’s\n",
      "integrity and validity.\n",
      "\n",
      "\n",
      "\n",
      "Verifying software package download integrity\n",
      "Typically any downloaded software packages will be supplied with a checksum value (usually MD5 or SHA256)\n",
      "and you should check that this checksum is correct after upload to the cluster to verify the integrity\n",
      "of the uploaded files.\n",
      "An example of checking the integrity of the Make RPM is shown below using the\n",
      "md5sum and sha256sum commands:\n",
      "[user@node004 [stanage] yumpackages]$ md5sum make-3.82-24.el7.x86_64.rpm\n",
      "c678cfe499cd64bae54a09b43f600231  make-3.82-24.el7.x86_64.rpm\n",
      "[user@node004 [stanage] yumpackages]$ sha256sum make-3.82-24.el7.x86_64.rpm\n",
      "d4829aff887b450f0f3bd307f782e062d1067ca4f95fcad5511148679c14a668  make-3.82-24.el7.x86_64.rpm\n",
      "\n",
      "\n",
      "At this stage if being thorough you should check that any vendor or package maintainer signatures on\n",
      "the downloaded binary packages are valid.\n",
      "If the vendor or maintainer has supplied a tarball or similar with an associated signature file (typically\n",
      "packagename.tar.gz.asc  or packagename.tar.gz.sig) then you can use gpg to check if it is valid as\n",
      "demonstrated below with the GNU Make project’s source tarball:\n",
      "[user@node004 [stanage] make]$ gpg --verify make-4.3.tar.gz.sig make-4.3.tar.gz\n",
      "gpg: Signature made Sun 19 Jan 2020 22:24:43 GMT using RSA key ID DB78137A\n",
      "gpg: Good signature from \"Paul D. Smith <paul@mad-scientist.net>\"\n",
      "gpg:                 aka \"Paul D. Smith <psmith@gnu.org>\"\n",
      "gpg: WARNING: This key is not certified with a trusted signature!\n",
      "gpg:          There is no indication that the signature belongs to the owner.\n",
      "Primary key fingerprint: 6D4E EB02 AD83 4703 510B  1176 80CB 727A 20C7 9BB2\n",
      "Subkey fingerprint: B250 8A90 102F 8AE3 B12A  0090 DEAC CAAE DB78 137A\n",
      "\n",
      "\n",
      "The GNU Make project documentation shows Paul D. Smith as the project maintainer and\n",
      "navigating to this personal site, http://make.mad-scientist.net/switching-gpg-keys/, shows\n",
      "the matching primary key fingerprint as expected and you can proceed with installing from\n",
      "the package.\n",
      "\n",
      "Warning\n",
      "Because we have not set Paul’s public key as trusted and signed his public key with our own\n",
      "private key, GPG will warn you that while the package is signed with Paul’s key, his key is not trusted\n",
      "nor can it verify it was indeed Paul’s key in the first place.\n",
      "As it is possible for anyone to generate a signing key with anyone’s name or email, you must verify\n",
      "the public key that signed the package is Paul’s e.g. by confirming with an alternate source such\n",
      "as his website which publishes the expected fingerprint.\n",
      "In order for GPG to trust the signature, several further commands are needed to add the key to your key\n",
      "chain, mark it as trusted then sign it with your own private key to establish a chain of trust / confirm\n",
      "to GPG you trust the key and have verified that it was indeed generated by Paul.\n",
      "As this has several steps and is required it is out of scope of this page.\n",
      "\n",
      "If you know that the vendor or maintainer already signs their other releases into the Centos repository\n",
      "and has supplied you an RPM then alternatively you can check signatures as detailed previously.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unpacking your binaries\n",
      "Unpacking binaries is typically an easy process but will depend on how they have been packaged, examples\n",
      "of unpacking an RPM and a Tarball are given below.\n",
      "\n",
      "\n",
      "\n",
      "Unpacking an RPM\n",
      "Unpacking an RPM is achieved by using the rpm2cpio and cpio commands in concert as shown below.\n",
      "This will unpackage the RPM into the current directory following a localised structure which would\n",
      "otherwise be where this package would be installed conventionally.\n",
      "i.e. ./usr/bin/gmake rather than /usr/bin/gmake\n",
      "\n",
      "The output below has been truncated to save space as indicated by *SNIP*.\n",
      "[user@node004 [stanage] yumpackages]$ rpm2cpio make-3.82-24.el7.x86_64.rpm | cpio -idmv\n",
      "./usr/bin/gmake\n",
      "./usr/bin/make\n",
      "./usr/share/doc/make-3.82\n",
      "./usr/share/doc/make-3.82/AUTHORS\n",
      "./usr/share/doc/make-3.82/COPYING\n",
      "./usr/share/doc/make-3.82/NEWS\n",
      "./usr/share/doc/make-3.82/README\n",
      "*SNIP*\n",
      "./usr/share/info/make.info-1.gz\n",
      "./usr/share/info/make.info-2.gz\n",
      "./usr/share/info/make.info.gz\n",
      "./usr/share/man/man1/gmake.1.gz\n",
      "./usr/share/man/man1/make.1.gz\n",
      "2278 blocks\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unpacking a tarball\n",
      "Unpacking a tarball is straightforward and is achieved using the tar command. Typically tarballs will be\n",
      "compressed with either GZip (tar.gz) or BZip (tar.bz2) and can be decompressed into the current\n",
      "directory by using the matching tar command arguments.\n",
      "For GZip compression the format of this command is:\n",
      "[user@node004 [stanage] tarpackages]$ tar -xzf mytarball.tar.gz\n",
      "\n",
      "\n",
      "For BZip compression the format of this command is:\n",
      "[user@node004 [stanage] tarpackages]$ tar -xjf mytarball.tar.bz2\n",
      "\n",
      "\n",
      "No example of this process is shown as the commands do not have terminal output unless there is an error.\n",
      "\n",
      "\n",
      "\n",
      "Making your binaries available in the shell\n",
      "At this stage you can typically move the unpackaged binaries as desired and any executables (in ./bin)\n",
      "or libraries (typically in ./lib and ./lib64 ) can be added to PATH or LD_LIBRARY_PATH\n",
      "using one of the two methodologies mentioned in the\n",
      "Making installed software available to execute section.\n",
      "\n",
      "\n",
      "\n",
      "Installing software by compiling from source\n",
      "\n",
      "Downloading the source code\n",
      "The first step of completing and installation from source on the clusters is to download\n",
      "the source code. In general there are few methods for downloading the source code for a project\n",
      "which will be detailed below.\n",
      "Typically source code will be made available from the maintainer’s FTP/HTTP servers or mirrors in the form\n",
      "of a compressed tarball or hosted on their chosen version control system site such as\n",
      "Github , Gitlab ,\n",
      "Atlassian Bitbucket  and GNU Savannah\n",
      "among many others.\n",
      "\n",
      "\n",
      "Downloading source Tarballs\n",
      "Downloading a source tarball is typically straightforwardand you can simply navigate to a package\n",
      "maintainer’s website, go into the download area and then download a tarball and it’s signature file if\n",
      "available.\n",
      "For example, the GNU make project download area can be found at https://ftp.gnu.org/gnu/make/ or on one of\n",
      "the numerous mirror websites.\n",
      "You may be able to use the wget command to download this directly to the cluster or may have to\n",
      "transfer this manually using SCP or similar. Once downloaded you should verify the software download’s\n",
      "integrity and validity.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Verifying software package download integrity\n",
      "Typically any downloaded software packages will be supplied with a checksum value (usually MD5 or SHA256)\n",
      "and you should check that this checksum is correct after upload to the cluster to verify the integrity\n",
      "of the uploaded files.\n",
      "An example of checking the integrity of the Make RPM is shown below using the\n",
      "md5sum and sha256sum commands:\n",
      "[user@node004 [stanage] yumpackages]$ md5sum make-3.82-24.el7.x86_64.rpm\n",
      "c678cfe499cd64bae54a09b43f600231  make-3.82-24.el7.x86_64.rpm\n",
      "[user@node004 [stanage] yumpackages]$ sha256sum make-3.82-24.el7.x86_64.rpm\n",
      "d4829aff887b450f0f3bd307f782e062d1067ca4f95fcad5511148679c14a668  make-3.82-24.el7.x86_64.rpm\n",
      "\n",
      "\n",
      "At this stage if being thorough you should check that any vendor or package maintainer signatures on\n",
      "the downloaded binary packages are valid.\n",
      "If the vendor or maintainer has supplied a tarball or similar with an associated signature file (typically\n",
      "packagename.tar.gz.asc  or packagename.tar.gz.sig) then you can use gpg to check if it is valid as\n",
      "demonstrated below with the GNU Make project’s source tarball:\n",
      "[user@node004 [stanage] make]$ gpg --verify make-4.3.tar.gz.sig make-4.3.tar.gz\n",
      "gpg: Signature made Sun 19 Jan 2020 22:24:43 GMT using RSA key ID DB78137A\n",
      "gpg: Good signature from \"Paul D. Smith <paul@mad-scientist.net>\"\n",
      "gpg:                 aka \"Paul D. Smith <psmith@gnu.org>\"\n",
      "gpg: WARNING: This key is not certified with a trusted signature!\n",
      "gpg:          There is no indication that the signature belongs to the owner.\n",
      "Primary key fingerprint: 6D4E EB02 AD83 4703 510B  1176 80CB 727A 20C7 9BB2\n",
      "Subkey fingerprint: B250 8A90 102F 8AE3 B12A  0090 DEAC CAAE DB78 137A\n",
      "\n",
      "\n",
      "The GNU Make project documentation shows Paul D. Smith as the project maintainer and\n",
      "navigating to this personal site, http://make.mad-scientist.net/switching-gpg-keys/, shows\n",
      "the matching primary key fingerprint as expected and you can proceed with installing from\n",
      "the package.\n",
      "\n",
      "Warning\n",
      "Because we have not set Paul’s public key as trusted and signed his public key with our own\n",
      "private key, GPG will warn you that while the package is signed with Paul’s key, his key is not trusted\n",
      "nor can it verify it was indeed Paul’s key in the first place.\n",
      "As it is possible for anyone to generate a signing key with anyone’s name or email, you must verify\n",
      "the public key that signed the package is Paul’s e.g. by confirming with an alternate source such\n",
      "as his website which publishes the expected fingerprint.\n",
      "In order for GPG to trust the signature, several further commands are needed to add the key to your key\n",
      "chain, mark it as trusted then sign it with your own private key to establish a chain of trust / confirm\n",
      "to GPG you trust the key and have verified that it was indeed generated by Paul.\n",
      "As this has several steps and is required it is out of scope of this page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Unpacking a tarball\n",
      "Unpacking a tarball is straightforward and is achieved using the tar command. Typically tarballs will be\n",
      "compressed with either GZip (tar.gz) or BZip (tar.bz2) and can be decompressed into the current\n",
      "directory by using the matching tar command arguments.\n",
      "For GZip compression the format of this command is:\n",
      "[user@node004 [stanage] tarpackages]$ tar -xzf mytarball.tar.gz\n",
      "\n",
      "\n",
      "For BZip compression the format of this command is:\n",
      "[user@node004 [stanage] tarpackages]$ tar -xjf mytarball.tar.bz2\n",
      "\n",
      "\n",
      "No example of this process is shown as the commands do not have terminal output unless there is an error.\n",
      "With the files now decompressed and available on the local file system you are ready to compile your\n",
      "software.\n",
      "\n",
      "\n",
      "\n",
      "Downloading source code with Git\n",
      "Downloading source code with Git is straightforward with the Git program already installed on the clusters.\n",
      "Once you have located the source code repository of interest you need only clone it to your local filesystem.\n",
      "An example of this process is shown with the GNU Make project. The GNU make project source code is hosted at\n",
      "https://git.savannah.gnu.org/cgit/make.git . Opening this page in the web browser will detail some important\n",
      "infomation needed in order to download and select the version of Make we are interested in.\n",
      "First clone the project using Git and the .git URL above as follows:\n",
      "[user@login1 [stanage] make-git]$ git clone https://git.savannah.gnu.org/git/make.git\n",
      "Cloning into 'make'...\n",
      "remote: Counting objects: 16331, done.\n",
      "remote: Compressing objects: 100% (3434/3434), done.\n",
      "remote: Total 16331 (delta 12822), reused 16331 (delta 12822)\n",
      "Receiving objects: 100% (16331/16331), 5.07 MiB | 2.79 MiB/s, done.\n",
      "Resolving deltas: 100% (12822/12822), done.\n",
      "\n",
      "\n",
      "This has cloned the latest version of the master branch into our local filesystem. Now we can instruct Git\n",
      "to checkout a specific version of Make via tags after entering the subdirectory that has been cloned.\n",
      "The available tags and branches will be shown on the source code repository webpage.\n",
      "[user@login1 [stanage] make-git]$ cd make\n",
      "[user@login1 [stanage] make]$ git checkout tags/4.3\n",
      "Note: checking out 'tags/4.3'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b new_branch_name\n",
      "\n",
      "HEAD is now at f430a65... GNU Make release 4.3\n",
      "\n",
      "\n",
      "The files on the local file system are now version 4.3, have been cloned over HTTPS and Git will have\n",
      "ensured the integrity of the downloaded files automatically. You are now able to compile your software.\n",
      "\n",
      "\n",
      "\n",
      "Compiling your source code into binaries\n",
      "Compiling from source is normally straightforward assuming that the prerequisites that a software package\n",
      "has are fulfilled correctly.\n",
      "Care must  be taken to read through the documentation provided in the\n",
      "software package files which are usually called README or INSTALL in the top level directory of\n",
      "the downloaded files. These files will dictate what specific instructions, compilers, build systems and\n",
      "versions are required for a successful compile.\n",
      "With this in mind, the process is very similar for most packages and will require you to first module\n",
      "load appropriate versions of GCC and / or CMake, potentially run a specific script (e.g. ./autogen.sh or\n",
      "./build), configure the build options and then compile the source code.\n",
      "e.g. compiling a more modern version of the make program on the Stanage cluster:\n",
      "\n",
      "Note\n",
      "Make is a tool which controls the generation of executables and other non-source files of a program\n",
      "from the program’s source files.\n",
      "Below shows the make program provided by the base operating system using GCC 8.2 to compile a more\n",
      "modern version of itself. This may seem quirky or recursive but is normal and will not lead to\n",
      "conflicts or issues.\n",
      "\n",
      "[user@node001 [stanage] make]$ cd make\n",
      "[user@node001 [stanage] make]$ mkdir ./build && cd ./build\n",
      "[user@node001 [stanage] make]$ module load GCC/12.2.0\n",
      "[user@node001 [stanage] make]$ ../configure --prefix=$HOME/software/installed/make\n",
      "[user@node001 [stanage] make]$ make -j $NSLOTS\n",
      "[user@node001 [stanage] make]$ make -j $NSLOTS check\n",
      "[user@node001 [stanage] make]$ make -j $NSLOTS install\n",
      "\n",
      "\n",
      "\n",
      "A build directory is made and then used to keep the source files unpolluted.\n",
      "The ../configure script is called from the directory above with the --prefix option set\n",
      "to where we want the installed files to be located.\n",
      "The make program provided the base operating system is then called 3 times. The first instance\n",
      "calls the compiler to compile the code, the second instance runs the maintainers’ check scripts to\n",
      "verify successful compilation and the final instance is called to then install the files.\n",
      "The -j $NSLOTS argument supplied to make instructs make to use multiple cores with the\n",
      "$NSLOTS variable containing the number of cores currently available in the requested interactive\n",
      "or batch session.\n",
      "\n",
      "Warning\n",
      "Care should be taken to observe the log output generated by this process to verify successful compilation\n",
      "and any indicated warnings or failed checks which could negatively affect your work or result in hard to\n",
      "diagnose unexpected behaviour.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making your compiled binaries available in the shell\n",
      "At this stage you can typically move the generated binaries as desired and any executables (in ./bin)\n",
      "or libraries (typically in ./lib and ./lib64 ) can be added to the PATH or LD_LIBRARY_PATH\n",
      "using one of the two methodologies mentioned in the following section.\n",
      "\n",
      "\n",
      "\n",
      "Making installed software available to execute\n",
      "Software on the HPC cluster can be made available using one of the two methods below:\n",
      "using your .bashrc file or making a custom module file (preferred) to enable multiple\n",
      "versions of the same software without conflicts.\n",
      "\n",
      "\n",
      "\n",
      "The .bashrc file and its purpose\n",
      "\n",
      "Caution\n",
      "Editing the .bashrc file can lead to a corrupted shell environment which can leave\n",
      "you unable to login or run programs.\n",
      "Please take care if editing this file and consider using the\n",
      "modules system to add directories to the PATH and\n",
      "LD_LIBRARY_PATH to avoid inadvertent mistakes.\n",
      "If you find your shell environment is behaving oddly, programs are no longer available and\n",
      "you suspect you may have corrupted your shell environment by editing the .bashrc file you\n",
      "can reset it with the command resetenv then logging out and back in.\n",
      "\n",
      "The .bashrc file is a hidden script file located in a user’s home directory which runs\n",
      "when the user logs in using the bash shell. The contents of .bashrc can be changed to define\n",
      "functions, command aliases, and customize the bash shell to the user’s liking.\n",
      "As this file is executed when the user logs in, it can be customised to add additional directories\n",
      "to the PATH and LD_LIBRARY_PATH in order to make software available to the shell.\n",
      "Adding a directory such as a personal installation directory with executables and libraries can be\n",
      "achieved as shown in the next section.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making software available via the .bashrc file\n",
      "Software can be made available using your .bashrc file and adding/editing any\n",
      "relevant environment variables.\n",
      "\n",
      "Caution\n",
      "We do not recommend editing your .bashrc file as this could result in corrupting your\n",
      "shell environment. If possible make use of the modules system.\n",
      "\n",
      "Assuming you have installed your software in a folder with the path $HOME/software/installs/mysoftware\n",
      "and the software has added a bin and lib folder. You would adjust your file to be:\n",
      "export PATH=$HOME/software/installs/mysoftware/bin:$PATH\n",
      "export LD_LIBRARY_PATH=$HOME/software/installs/mysoftware/lib:$LD_LIBRARY_PATH\n",
      "\n",
      "\n",
      "If you are installing libraries and software that are dependencies, using 64 bit software,\n",
      "need to set a variable for a license server/file etc… you may also need to use other environment\n",
      "variables pointing at different paths e.g.\n",
      "export LD_LIBRARY_PATH=$HOME/software/installs/mysoftware/lib:$LD_LIBRARY_PATH\n",
      "export LD_LIBRARY_PATH=$HOME/software/installs/mysoftware/lib64:$LD_LIBRARY_PATH\n",
      "\n",
      "\n",
      "export LIBRARY_PATH=$HOME/software/installs/mysoftware/lib:$LIBRARY_PATH\n",
      "export LIBRARY_PATH=$HOME/software/installs/mysoftware/lib64:$LIBRARY_PATH\n",
      "\n",
      "\n",
      "export PKG_CONFIG_PATH=$HOME/software/installs/mysoftware/lib/pkgconfig:$PKG_CONFIG_PATH\n",
      "export PKG_CONFIG_PATH=$HOME/software/installs/mysoftware/lib64/pkgconfig:$PKG_CONFIG_PATH\n",
      "export PKG_CONFIG_PATH=$HOME/software/installs/mysoftware/share/pkgconfig:$PKG_CONFIG_PATH\n",
      "\n",
      "\n",
      "export ACLOCAL_PATH=$HOME/software/installs/mysoftware/share/aclocal:$ACLOCAL_PATH\n",
      "\n",
      "\n",
      "export CMAKE_PREFIX_PATH=$HOME/software/installs/mysoftware/:$CMAKE_PREFIX_PATH\n",
      "\n",
      "\n",
      "export CPLUS_INCLUDE_PATH=$HOME/software/installs/mysoftware/include:$CPLUS_INCLUDE_PATH\n",
      "export CPATH=$HOME/software/installs/mysoftware/include:$CPATH\n",
      "\n",
      "\n",
      "export MY_SOFTWARE_LICENSE_PATH=$HOME/software/licenses/mysoftware/license.lic\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Environment ‘Modules’ and their purpose\n",
      "‘Environment Modules’ are the mechanism by which much of the software is made available to the users\n",
      "of the Stanage and Bessemer clusters. You are able to load and unload modules which make specific\n",
      "configurations of software available in a structured way which can avoid conflicts between different\n",
      "versions of the same software.\n",
      "They do this by adding and removing software to the the PATH and LD_LIBRARY_PATH environment\n",
      "variables as well as set any additional required environment varibles, configuration or license files using\n",
      "the module load or  module unload functionality.\n",
      "Module files are written in Lua on Stanage and TCL on Bessemer. To see examples, check the module paths with echo $MODULEPATH\n",
      "to get an idea of what these should look like.\n",
      "Further detail on the environment modules system in use on the clusters can be found on the\n",
      "modules page.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Making software available via a custom module file\n",
      "If you wish to use the modules system with personal\n",
      "module files you can add a directory called modules to your home directory\n",
      "mkdir $HOME/modules and populate this with your own module files.\n",
      "To make these available automatically you can then add the module use $HOME/modules\n",
      "command to your .bashrc file.\n",
      "\n",
      "StanageBessemerYou can generate a basic module file using the basic Lua directives to set local variables,\n",
      "export these to your shell with the setenv function and prepend paths to your existing environment\n",
      "variables with the prepend_path function.\n",
      "\n",
      "Warning\n",
      "Module files are not aware of bash shell variables unless you import them using the os.getenv function and set a Lua variable based on them.\n",
      "e.g. the following example imports our shell environment HOME variable and sets the Lua\n",
      "HOME variable with it. The Lua HOME variable is used to set the Lua variable MY_PROGRAM_DIR\n",
      "(the software’s installation directory). The Lua MY_PROGRAM_DIR variable is then used to add the program’s\n",
      "bin directory to your shell environment PATH variable with the prepend_path function.\n",
      "-- Import the shell environment HOME variable into a Lua variable.\n",
      "local HOME = os.getenv(\"HOME\")\n",
      "\n",
      "-- Set a local Lua variable for the program directory.\n",
      "local MY_PROGRAM_DIR = HOME .. \"/software/installs/my_new_program\"\n",
      "\n",
      "-- Export the program directory to the shell environment.\n",
      "setenv(\"MY_PROGRAM_DIR\", MY_PROGRAM_DIR)\n",
      "\n",
      "-- Prepend the program's bin directory to the shell environment PATH variable.\n",
      "prepend_path(\"PATH\", MY_PROGRAM_DIR .. \"/bin\")\n",
      "\n",
      "\n",
      "\n",
      "Much like using a .bashrc file with the export command, we can add the required variables and directives\n",
      "to a custom module file. For example, if called CustomModule and saved in $HOME/modules/ may\n",
      "look something like:\n",
      "------------------------------------------------------------------------------------------------\n",
      "/users/my_username/modules/my_new_program.lua:\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "-- Provide help text for the module.\n",
      "help([[\n",
      "Description\n",
      "===========\n",
      "Makes my newly installed program available.\n",
      "\n",
      "More information\n",
      "================\n",
      "- Homepage: https://www.my-new-programme.com\n",
      "]])\n",
      "\n",
      "-- Describe the module.\n",
      "whatis(\"Description: Makes my newly installed program available.\")\n",
      "whatis(\"Homepage: https://www.my-new-programme.com\")\n",
      "whatis(\"URL: https://www.my-new-programme.com\")\n",
      "\n",
      "-- Specify a conflicting module.\n",
      "conflict(\"CustomModule\")\n",
      "\n",
      "-- Load any dependencies.\n",
      "load(\"GCC/10.2\")\n",
      "load(\"CMake/3.18.4-GCCcore-10.2.0\")\n",
      "\n",
      "-- Set a program root directory Lua variable MY_PROGRAM_DIR to simplify prepend_path directives.\n",
      "-- Reminder: setting an environment variable with setenv does not set the equivalent Lua variable!\n",
      "-- Reminder: setting a Lua variable does not set the equivalent shell environment variable either!\n",
      "-- Note no trailing slash is required for MY_PROGRAM_DIR as we are using a / on the prepend_path directives.\n",
      "local MY_PROGRAM_DIR = \"/users/my_username/software/installs/my_new_program\"\n",
      "setenv(\"MY_PROGRAM_DIR\", MY_PROGRAM_DIR)\n",
      "setenv(\"MY_SOFTWARE_LICENSE_PATH\", \"/users/my_username/software/licenses/mysoftware/license.lic\")\n",
      "\n",
      "-- Add directories to environment variables.\n",
      "prepend_path(\"PATH\", pathJoin(MY_PROGRAM_DIR, \"bin\"))\n",
      "prepend_path(\"LIBRARY_PATH\", pathJoin(MY_PROGRAM_DIR, \"lib\"))\n",
      "prepend_path(\"LD_LIBRARY_PATH\", pathJoin(MY_PROGRAM_DIR, \"lib\"))\n",
      "prepend_path(\"PKG_CONFIG_PATH\", pathJoin(MY_PROGRAM_DIR, \"lib/pkgconfig\"))\n",
      "prepend_path(\"CMAKE_PREFIX_PATH\", MY_PROGRAM_DIR)\n",
      "\n",
      "\n",
      "You can generate a basic module file using the basic TCL directives to set variables,\n",
      "export these to your shell with setenv and prepend paths to your existing environment\n",
      "variables with prepend-path.\n",
      "\n",
      "Warning\n",
      "Module files are not aware of bash shell variables unless you import them from the env array\n",
      "and set a TCL variable based on them.\n",
      "e.g. the following example imports our shell environment HOME variable and sets the TCL\n",
      "HOME variable with it. The TCL HOME variable is used to set the TCL variable MY_PROGRAM_DIR\n",
      "(the software’s installation directory). The TCL MY_PROGRAM_DIR variable is then used to add the program’s\n",
      "bin directory to your shell environment PATH variable with the prepend-path directive.\n",
      "set             HOME                $::env(HOME)\n",
      "set             MY_PROGRAM_DIR      $HOME/software/installs/my_new_program\n",
      "prepend-path    PATH                $MY_PROGRAM_DIR/bin\n",
      "\n",
      "\n",
      "\n",
      "Much like using a .bashrc file with the export command, we can add the required variables and directives\n",
      "to a custom module file. For example, if called CustomModule and saved in $HOME/modules/ may\n",
      "look something like:\n",
      "#%Module1.0#####################################################################\n",
      "##\n",
      "## My newly installed program module file\n",
      "##\n",
      "\n",
      "## Module file logging - this is TUoS cluster specific!\n",
      "source /usr/local/etc/module_logging.tcl\n",
      "##\n",
      "\n",
      "proc ModulesHelp { } {\n",
      "        puts stderr \"Makes my newly installed program available.\"\n",
      "}\n",
      "\n",
      "module-whatis   \"Makes my newly installed program available.\"\n",
      "\n",
      "## Load any dependencies\n",
      "\n",
      "module load GCC/10.2\n",
      "module load CMake/3.18.4-GCCcore-10.2.0\n",
      "\n",
      "## Set a program root directory TCL variable MY_PROGRAM_DIR to simplify prepend-path directives.\n",
      "## **Reminder** setting an environment variable with setenv does not set the equivalent TCL variable!\n",
      "## **Reminder** setting a TCL variable does not set the equivalent shell environment variable either!\n",
      "## Note no trailing slash is required for MY_PROGRAM_DIR as we are using a / on the prepend-path directives.\n",
      "\n",
      "set             MY_PROGRAM_DIR              /home/my_username/software/installs/my_new_program\n",
      "setenv          MY_PROGRAM_DIR              $MY_PROGRAM_DIR\n",
      "setenv          MY_SOFTWARE_LICENSE_PATH    /home/my_username/software/licenses/mysoftware/license.lic\n",
      "\n",
      "prepend-path    PATH               $MY_PROGRAM_DIR/bin\n",
      "prepend-path    LIBRARY_PATH       $MY_PROGRAM_DIR/lib\n",
      "prepend-path    LD_LIBRARY_PATH    $MY_PROGRAM_DIR/lib\n",
      "prepend-path    PKG_CONFIG_PATH    $MY_PROGRAM_DIR/lib/pkgconfig\n",
      "prepend-path    CMAKE_PREFIX_PATH  $MY_PROGRAM_DIR\n",
      "\n",
      "\n",
      "\n",
      "Hint\n",
      "If you get warnings about missing file paths please ensure the file path exists and/or you have not made a mistake\n",
      "when defining your TCL variables. (Remember the difference between set and setenv directives and that one\n",
      "does not set the other.)\n",
      "\n",
      "\n",
      "If the module use command (module use $HOME/modules) is applied in your .bashrc file you could now load this module by running:\n",
      "$ module load CustomModule\n",
      "\n",
      "\n",
      "And unload with:\n",
      "$ module unload CustomModule\n",
      "\n",
      "\n",
      "Modulefiles make it easy to add many versions of the same software easily via duplication and simple editing\n",
      "without the risk of permanently corrupting your shell environment. Further info on the modules system can be\n",
      "found on the modules page.\n",
      "\n",
      "\n",
      "\n",
      "Why should I install from source?\n",
      "\n",
      "Further performance optimisations may be available for your chosen cluster / computer.\n",
      "Dependencies may not be available with the versions required for a binary installation.\n",
      "The version of the software you desire has no precompiled binaries available.\n",
      "Your machine architecture does not have any precompiled binaries available.\n",
      "\n",
      "\n",
      "\n",
      "What alternative methods exist?\n",
      "\n",
      "Conda\n",
      "Pip\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/flight-desktop.html'}\n",
      "\n",
      "\n",
      "\n",
      "Graphical Sessions (With Flight) on Stanage\n",
      "Graphical desktop access to an interactive session can be achieved using Flight Desktop and TigerVNC. See below for simple usage instructions.\n",
      "\n",
      "Usage instructions\n",
      "Download and install TigerVNC on your machine.  TigerVNC is a program that allows you to efficiently view graphical programs on one computer that are actually running on another computer.\n",
      "After connecting to Stanage (see Establishing a SSH connection),  start an interactive session with the following command:\n",
      "srun --pty bash -i\n",
      "\n",
      "\n",
      "\n",
      "Tip\n",
      "You may wish to consider requesting more resources for your session, please see requesting an interactive session on slurm\n",
      "\n",
      "Initialise and start Flight Desktop:\n",
      "flight start # Start the Flight environment to enable further Flight commands\n",
      "flight desktop verify xfce   # only do this once\n",
      "flight desktop start --geometry 1800x1000 xfce\n",
      "\n",
      "\n",
      "\n",
      "Warning\n",
      "If you get a warning about “missing prerequisites”, please ensure you have requested an interactive session and your terminal shows node001 or node002.\n",
      "In the case of “missing prerequisites” warnings while in an interactive session, please see troubleshooting:\n",
      "\n",
      "The following is example output:\n",
      "  Starting a 'xfce' desktop session:\n",
      "\n",
      " ✅ Starting session\n",
      "\n",
      "  A 'xfce' desktop session has been started.\n",
      "\n",
      "  == Session details ==\n",
      "\n",
      "    Identity: 8db560e9-d56e-4776-81d2-037c690279bc\n",
      "        Type: xfce\n",
      "     Host IP: 10.10.1.1\n",
      "    Hostname: node001\n",
      "        Port: 5911\n",
      "     Display: :11\n",
      "    Password: l7IdD9I0\n",
      "\n",
      "  This desktop session is not directly accessible from outside of your\n",
      "  cluster as it is running on a machine that only provides internal\n",
      "  cluster access.  In order to access your desktop session you will need\n",
      "  to perform port forwarding using 'ssh'.\n",
      "\n",
      "  Refer to 'flight desktop show 8db561e9' for more details.\n",
      "\n",
      "  If prompted, you should supply the following password: l7IdE9I0\n",
      "\n",
      "\n",
      "Get details of the desktop session using the command provided in the previous output:\n",
      "flight desktop show xxxx  # xxxx = session number\n",
      "\n",
      "\n",
      "The following is example output:\n",
      "      == Session details ==\n",
      "\n",
      "    Identity: 8db560e9-d56e-4776-81d2-037c690269bc\n",
      "        Type: xfce\n",
      "    Host IP: 10.10.1.1\n",
      "    Hostname: node001\n",
      "        Port: 5911\n",
      "    Display: :11\n",
      "    Password: l7IdD9I0\n",
      "\n",
      "  This desktop session is not directly accessible from outside of your\n",
      "  cluster as it is running on a machine that only provides internal\n",
      "  cluster access.  In order to access your desktop session you will need\n",
      "  to perform port forwarding using 'ssh':\n",
      "\n",
      "    ssh -L 5911:10.10.1.1:5911 te1st@\n",
      "\n",
      "  Once the ssh connection has been established, depending on your\n",
      "  client, you can connect to the session using one of:\n",
      "\n",
      "    vnc://te1st:l7IdD9I0@localhost:5911\n",
      "    localhost:5911\n",
      "    localhost:11\n",
      "\n",
      "  If, when connecting, you receive a warning as follows, try again with\n",
      "  a different port number, e.g. 5912, 5913 etc.:\n",
      "\n",
      "    channel_setup_fwd_listener_tcpip: cannot listen to port: 5911\n",
      "\n",
      "  If prompted, you should supply the following password: l7IdD9I0\n",
      "\n",
      "\n",
      "\n",
      "Tip\n",
      "\n",
      "Take note of the lines which have been highlighted above:\n",
      "a port number, (5911),\n",
      "an ssh command you run on your local machine which connects the VNC session to your local machine,\n",
      "a temporary VNC password (l7IdD9I0) which is valid only for your current VNC session.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Leave that terminal running. And, open another local terminal (ex. Windows PowerShell)\n",
      "entering the provided ssh command including cluster hostname after the @ sign. If prompted for password, enter your university account password (not the flight generated temporary VNC password which is valid only for the current VNC session).\n",
      "For example:\n",
      "ssh -L 5911:10.10.1.1:5911 te1st@stanage.shef.ac.uk\n",
      "\n",
      "\n",
      "This step will set up local port forwarding to the remote host (Stanage).\n",
      "On your local machine start the ‘VNC Viewer’ program that comes with TigerVNC (this is called vncviewer on Linux).  You should then see a dialog box like this:\n",
      "\n",
      "Caution\n",
      "Do not enter your normal university account password, use the temporary VNC password which was given in the flight desktop show command output.\n",
      "\n",
      "\n",
      "Using the previously given port number, enter the connection details into the TigerVNC dialog, for example:\n",
      "localhost:5911\n",
      "\n",
      "\n",
      "Click Connect, you will then be prompted for the temporary VNC password.\n",
      "You should now see a desktop within a window, as below. This desktop is running within the interactive session\n",
      "we requested on Stanage.\n",
      "\n",
      "You can click close on the error dialog. Then open a terminal at the bottom of the screen which will open inside the interactive session you first requested.\n",
      "When you are finished, close VNC Viewer then return to the terminal within which you started Flight Desktop and log out or continue with other tasks.\n",
      "\n",
      "Example\n",
      "An example, starting a MATLAB GUI in TigerVNC Viewer by entering the following commands into the terminal\n",
      "$ module load MATLAB/2023b\n",
      "$ matlab\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Troubleshooting\n",
      "\n",
      "You’re seeing a “missing prerequisites”: error message:\n",
      "Desktop type xfce has missing prerequisites:\n",
      "\n",
      "* Repository: EPEL\n",
      "Before this desktop type can be used, it must be prepared by your\n",
      "cluster administrator using the 'prepare' command, i.e.:\n",
      "\n",
      "  flight desktop prepare xfce\n",
      "\n",
      "\n",
      "In most cases, this can be resolved using the following command:\n",
      "mv ~/.local/share/flight/desktop ~/.local/share/flight/desktop_bk\n",
      "\n",
      "\n",
      "Next verify again:\n",
      "flight desktop verify xfce   # only do this once\n",
      "flight desktop start --geometry 1800x1000 xfce\n",
      "\n",
      "\n",
      "Then continue from here.\n",
      "\n",
      "\n",
      "The GUI is slow or unresponsive:\n",
      "You may wish to consider requesting more resources for your session, please see requesting an interactive session on slurm\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/hpcgateway.html'}\n",
      "\n",
      "\n",
      "\n",
      "HPC Gateway Service\n",
      "\n",
      "Caution\n",
      "The HPC Gateway service is not required to access the Stanage cluster, or any Sheffield University HPC clusters. Use of a\n",
      "VPN connection is the recommended method to use for off-site SSH access to the HPC clusters.\n",
      "Access to the HPC clusters via the HPC gateway service  will only be granted to users who are unable use the VPN with a valid reason.\n",
      "\n",
      "\n",
      "Service description\n",
      "The HPC Gateway service is provided to give access to the Sheffield University HPC clusters from off campus where usage of the VPN is not possible.\n",
      "This access is provided by a SSH gateway server which is configured to function as a SSH ‘jump host’ only. It only allows SSH jump host connections to the HPC clusters.\n",
      "It cannot:\n",
      "\n",
      "Access HPC filestores directly.\n",
      "Access research shared areas directly.\n",
      "Allow connections to other IT Services or departmental servers.\n",
      "Run an interactive SSH terminal session on the gateway server.\n",
      "\n",
      "\n",
      "\n",
      "Access conditions\n",
      "\n",
      "Access to the HPC SSH gateway service requires that you have an existing HPC account.\n",
      "Access requests for the HPC SSH gateway service require a valid justification. If usage of the SSL VPN without undue effort is possible for HPC access, your request will be denied.\n",
      "Access to the HPC SSH gateway service is on a case by case basis, upon request, via emailing research-it@sheffield.ac.uk.\n",
      "\n",
      "\n",
      "\n",
      "Overview of the connection process\n",
      "\n",
      "\n",
      "\n",
      "Specific usage examples\n",
      "\n",
      "Hint\n",
      "Usernames to connect with all HPC services will be the same as those you use to login to MUSE not the prefix on your email address.\n",
      "\n",
      "\n",
      "Access a HPC cluster via SSH:\n",
      "\n",
      "\n",
      "StanageBessemerssh -J [username]@hpcgw.shef.ac.uk [username]@stanage.shef.ac.uk\n",
      "\n",
      "\n",
      "ssh -J [username]@hpcgw.shef.ac.uk [username]@bessemer.shef.ac.uk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Transfer a file using SCP:\n",
      "\n",
      "scp -J [username]@hpcgw.shef.ac.uk [source path] [destination path]\n",
      "\n",
      "\n",
      "\n",
      "Transfer files using Rsync:\n",
      "\n",
      "rsync -av -e 'ssh -J [username]@hpcgw.shef.ac.uk' [source path] [destination path]\n",
      "\n",
      "\n",
      "\n",
      "Using WinSCP:\n",
      "\n",
      "New Session -> Advanced -> Connection -> Tunnel\n",
      "Select 'Connect through SSH tunnel'\n",
      "Hostname: 'hpcgw.shef.ac.uk'\n",
      "Port number: '22'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Configure MobaXterm:\n",
      "Edit 'Session Settings':\n",
      "Set 'SSH Use 2-factor authentication for SSH gateways'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Create a new session using MobaXterm:\n",
      "Select 'Network settings' tab within SSH Session settings\n",
      "Select 'Connect through SSH gateway (jump host)\n",
      "Gateway SSH server: 'hpcgw.shef.ac.uk'\n",
      "Port: '22'\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "When prompted to enter your Duo two-factor code either input a 6 digit code from your Duo device or enter ‘1’ for a push notification to be sent to your device.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/advanced_job_analysis_and%20_profiling.html'}\n",
      "\n",
      "-----------------------------------------------------\n",
      "{'source': 'https://docs.hpc.shef.ac.uk/en/latest/hpc/scheduler/drmaa.html'}\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional, Sequence, Union\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "\n",
    "# class CustomWebLoader(WebBaseLoader):\n",
    "#     def lazy_load(self) -> Iterator[Document]:\n",
    "#         \"\"\"Lazy load text from the url(s) in web_path.\"\"\"\n",
    "#         for path in self.web_paths:\n",
    "#             soup = self._scrape(path, bs_kwargs=self.bs_kwargs)\n",
    "\n",
    "#             # Find the div with itemprop=\"articleBody\"\n",
    "#             body = soup.find('div', role=\"main\") \n",
    "            \n",
    "#             if body is None:\n",
    "#                 break\n",
    "#             text = body.get_text(**self.bs_get_text_kwargs)\n",
    "#             metadata = _build_metadata(soup, path)\n",
    "#             yield Document(page_content=text, metadata=metadata)\n",
    "\n",
    "\n",
    "bs4_strainer = bs4.SoupStrainer(attrs={'role': 'main'})\n",
    "\n",
    "# Use the custom loader\n",
    "loader = WebBaseLoader(web_paths=urls, bs_kwargs={\"parse_only\": bs4_strainer})\n",
    "documents = loader.load()\n",
    "\n",
    "# Print the results\n",
    "for doc in documents:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content)\n",
    "\n",
    "    print(\"-----------------------------------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama3-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
